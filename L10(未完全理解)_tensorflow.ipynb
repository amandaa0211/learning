{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Âê≥_W10_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4yVNLvxMn1",
        "colab_type": "text"
      },
      "source": [
        "# Graph and Sessions\n",
        "In TensorFlow, the definition of computations is separated from their execution. First, we specify the operations by building a data flow graph in Python. Next, TensorFlow runs the graph with a Session using optimized C++ code. Let's import tensorflow first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmPhTowvxGMf",
        "colab_type": "code",
        "outputId": "55e2915f-995c-43fd-fab3-64cde7c49a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tempfile import gettempdir\n",
        "import urllib\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.0.0-beta1\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjDiyjj7xckG",
        "colab_type": "text"
      },
      "source": [
        "## Graph\n",
        "A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
        "\n",
        "\n",
        "*   tf.Operation: The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
        "*   tf.Tensor: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors.\n",
        "\n",
        "**Notes:** tf.Tensors do not have values, they are just handles to elements in the computation graph.\n",
        "\n",
        "\n",
        "## Sessions\n",
        "To evaluate tensors, instantiate a tf.Session object, informally known as a session. A session encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9MXC7bCxYzW",
        "colab_type": "code",
        "outputId": "82ba8a0b-6274-4ce7-c4c8-c7ba5c0b76f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# build a naive graph\n",
        "a_tensor = tf.constant(3., name=\"const3\")\n",
        "b_tensor = tf.constant(4., name=\"const4\")\n",
        "out_tensor = tf.add(a_tensor, b_tensor) \n",
        "print(a_tensor, b_tensor, out_tensor, sep=\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"const3:0\", shape=(), dtype=float32)\n",
            "Tensor(\"const4:0\", shape=(), dtype=float32)\n",
            "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpLxY4rxy_hW",
        "colab_type": "text"
      },
      "source": [
        "Notice that printing the tensors does not output the values 3.0, 4.0, and 7.0 as you might expect. The above statements only build the computation graph. These tf.Tensor objects just represent the results of the operations that will be run.\n",
        "**We need a tf.Session to run it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yes6LVyzdQV",
        "colab_type": "code",
        "outputId": "5175087d-8df9-40a0-cb20-79e0816f5f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sess = tf.Session() # create a session\n",
        "a, b, c = sess.run([a_tensor, b_tensor, out_tensor])\n",
        "print(\"a = {} \\nb = {} \\nc = {}\".format(a, b, c))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a = 3.0 \n",
            "b = 4.0 \n",
            "c = 7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Udj0z_P1RiV",
        "colab_type": "text"
      },
      "source": [
        "## Tensor\n",
        "A **tensor** is a generalization of vectors and matrices to potentially higher dimensions.\n",
        "\n",
        "When writing a TensorFlow program, the main object you manipulate and pass around is the tf.Tensor. **A tf.Tensor object represents a partially defined computation that will eventually produce a value.** TensorFlow programs work by first building a graph of tf.Tensor objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\n",
        "\n",
        "\n",
        "\n",
        "A *tf.Tensor* has the following properties:\n",
        "\n",
        "1. a data type (tf.float32, tf.int32, or tf.string, for example)\n",
        "2. a shape\n",
        "\n",
        "The **rank** of a tensor refers to the number of dimensions it has.\n",
        "\n",
        "The **shape** of a tensor speficies the array's length along each dimension.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# \n",
        "3. # a rank 0 tensor; a scalar with shape [],\n",
        "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
        "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
        "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n",
        "\n",
        "```\n",
        "\n",
        "Some types of tensors are special. \n",
        "The main ones are:\n",
        "\n",
        "\n",
        "\n",
        "*   tf.constant\n",
        "*   tf.Variable\n",
        "*   tf.placeholder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLSGCT3-6-7o",
        "colab_type": "text"
      },
      "source": [
        "## tf.constant\n",
        "\n",
        "We can create constants by passing lists or constants into the tf.constant function.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_KEz-YQ7gYM",
        "colab_type": "code",
        "outputId": "6e6d8ce9-616f-4c94-b249-38874b5e80b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "# constant of 1d tensor (vector)\n",
        "a = tf.constant([2, 2], dtype=tf.int32, name=\"vector\")\n",
        "# constant of 2x2 tensor (matrix)\n",
        "b = tf.constant([[0, 1], [2, 3]], name=\"matrix\")\n",
        "print(a, b, sep=\"\\n\")\n",
        "print('---------------------------')\n",
        "# a matrix with filled zeros\n",
        "c = tf.zeros([2, 3], tf.int32, name=\"zeros_matrix\") # [[0, 0, 0], [0, 0, 0]]\n",
        "# a matrix with filled ones\n",
        "d = tf.ones([2, 3], tf.int32, name=\"ones_matrix\") #  [[1, 1, 1], [1, 1, 1]]\n",
        "\n",
        "# create a tensor filled zeros/ones, with shape and type as input_tensor\n",
        "input_tensor = tf.constant([[1,1], [2,2], [3,3]], dtype=tf.float32)\n",
        "e = tf.zeros_like(input_tensor, name=\"zeros_like_matrix\")  #  [[0, 0], [0, 0], [0, 0]]\n",
        "f = tf.ones_like(input_tensor, name=\"ones_like_matrix\") # [[1, 1], [1, 1], [1, 1]]\n",
        "\n",
        "print(c, d, e, f, sep=\"\\n\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"vector:0\", shape=(2,), dtype=int32)\n",
            "Tensor(\"matrix:0\", shape=(2, 2), dtype=int32)\n",
            "---------------------------\n",
            "Tensor(\"zeros_matrix:0\", shape=(2, 3), dtype=int32)\n",
            "Tensor(\"ones_matrix:0\", shape=(2, 3), dtype=int32)\n",
            "Tensor(\"zeros_like_matrix:0\", shape=(3, 2), dtype=float32)\n",
            "Tensor(\"ones_like_matrix:0\", shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEog7Syf8sTS",
        "colab_type": "text"
      },
      "source": [
        "## tf.Variables\n",
        "Unlike a constant, a variable can be assigned to, so its value can be changed. Also, a constant's value is stored on the graph, whereas a variable's value is stored seperately. To declare a variable, we create a instance of *tf.get_variable.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2l-0L5d84tD",
        "colab_type": "code",
        "outputId": "893a67a6-901c-4cc5-fb67-99a508969fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# create a variable of vector\n",
        "vec_var = tf.get_variable(name=\"vector\", shape=[3],\n",
        "                          initializer=tf.ones_initializer)\n",
        "# create a variable of matrix\n",
        "mat_var = tf.get_variable(name=\"matrix\", shape=[5, 3],\n",
        "                          initializer=tf.random_normal_initializer)\n",
        "\n",
        "print(vec_var, mat_var, sep=\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'vector_1:0' shape=(3,) dtype=float32_ref>\n",
            "<tf.Variable 'matrix_1:0' shape=(5, 3) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT7Amwuu-qDy",
        "colab_type": "code",
        "outputId": "84e196fd-fec6-4153-f134-1439d28ad22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# instance of `tf.Variable`\n",
        "var = tf.Variable(2, name=\"scalar\")\n",
        "\n",
        "# we can assign new value to a variable\n",
        "var_times_two = var.assign(var * 2) # an operation that assigns value var*2 to var\n",
        "print(var, var_times_two, sep=\"\\n\")\n",
        "# constant value is not changable\n",
        "# the following code will casue error\n",
        "#c = tf.constant(0.)\n",
        "#c.assign(1.)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'scalar:0' shape=() dtype=int32_ref>\n",
            "Tensor(\"Assign:0\", shape=(), dtype=int32_ref)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1u06WnO_mw1",
        "colab_type": "text"
      },
      "source": [
        "Since constraint mentiond above, **all fixed value should be as type tf.constant, while trainable weights should be as type tf.Variable**\n",
        "\n",
        "**Before you can use a variable, it must be initialized**. If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables.\n",
        "\n",
        "To initialize all trainable variables in one go, before training starts, call ***tf.global_variables_initializer()***. This function returns a single operation responsible for initializing all variables in the tf.GraphKeys.GLOBAL_VARIABLES collection. Running this operation initializes all variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1pSfP7s_RDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess=tf.Session()\n",
        "variable_init_op = tf.global_variables_initializer() # an operation\n",
        "sess.run(variable_init_op) # initialize the variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqURIfsZ2M3",
        "colab_type": "text"
      },
      "source": [
        "## tf.placeholder\n",
        " A graph can be parameterized to accept external inputs, known as **placeholders**. A **placeholder** is a promise to provide a value later, like a function argument.\n",
        "\n",
        "`tf.placeholder(dtype, shape=None, name=None)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vLbp8sl__i9",
        "colab_type": "code",
        "outputId": "c9d3d8ea-35ca-4c67-c026-2c3d1942e6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a_placeholder = tf.placeholder(tf.float32, shape=[None, 3], name=\"a\")\n",
        "print(a_placeholder)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"a:0\", shape=(?, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK2jIOrTalxZ",
        "colab_type": "code",
        "outputId": "ca90b72f-5fe5-47d7-889a-4533184cd0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sess = tf.Session() \n",
        "a = sess.run(a_placeholder, feed_dict={a_placeholder: [[1, 2, 3], [4, 5, 6]]})\n",
        "print(a)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvx0TQB4aqw9",
        "colab_type": "text"
      },
      "source": [
        "## Use an example to summarize above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdYzT7tpap12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a basic graph that demos the basic tensorflow concepts\n",
        "with tf.Graph().as_default() as g:\n",
        "    # a constant tensor with rank = 0\n",
        "    scalar_tensor = tf.constant(5., name=\"scalar\") \n",
        "    \n",
        "    # a vector tensor with rank = 1, and filled with random values\n",
        "    vector_tensor = tf.random_normal(shape=[5], name=\"vector\") \n",
        "    \n",
        "    # tensorflow supports broadcast\n",
        "    broadcast_with_scalar = vector_tensor + scalar_tensor \n",
        "    \n",
        "    # use placeholder to get values in runtime\n",
        "    x_input = tf.placeholder(tf.float32, shape=[None, 5], name=\"input\")\n",
        "    feature_dims = x_input.shape[1] #5\n",
        "    \n",
        "    # a matrix variable with rank = 2.\n",
        "    matrix_variable = tf.get_variable(\"matrix\",\n",
        "                                      shape=[feature_dims, 2],\n",
        "                                      initializer=tf.ones_initializer) \n",
        "    mul_with_matrix = tf.matmul(x_input, matrix_variable, name=\"output\")\n",
        "    \n",
        "    var_init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPkoAefnanPv",
        "colab_type": "code",
        "outputId": "24729897-c1a5-445b-ef1f-93deb7ba2a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "feed_data = np.random.randint(5, size=[5, 5])\n",
        "\n",
        "# Under the scope of session, we can run the value of tensor in default graph\n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(var_init_op) # must initialize the variables\n",
        "    \n",
        "    scalar, vector, broadcast = sess.run([scalar_tensor,\n",
        "                                          vector_tensor,\n",
        "                                          broadcast_with_scalar])\n",
        "    print(\"[scalar]\\n {} \\n[vector]\\n {} \\n[broadcast]\\n {}\".format(scalar,\n",
        "                                                                    vector,\n",
        "                                                                    broadcast))\n",
        "    \n",
        "    x, m, out = sess.run([x_input, matrix_variable, mul_with_matrix],\n",
        "                         feed_dict={x_input: feed_data})\n",
        "    print(\"[input]\\n {} \\n[matrix]\\n {} \\n[output]\\n {}\".format(x, m, out)) #m.shape()=5*2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[scalar]\n",
            " 5.0 \n",
            "[vector]\n",
            " [ 0.74704355 -0.54719234 -0.34323072 -1.3947614   1.1224443 ] \n",
            "[broadcast]\n",
            " [5.7470436 4.4528074 4.6567693 3.6052384 6.122444 ]\n",
            "[input]\n",
            " [[0. 1. 3. 3. 1.]\n",
            " [1. 4. 4. 0. 1.]\n",
            " [2. 3. 4. 3. 4.]\n",
            " [3. 1. 2. 0. 0.]\n",
            " [0. 1. 4. 4. 1.]] \n",
            "[matrix]\n",
            " [[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]] \n",
            "[output]\n",
            " [[ 8.  8.]\n",
            " [10. 10.]\n",
            " [16. 16.]\n",
            " [ 6.  6.]\n",
            " [10. 10.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqUtUDileUkI",
        "colab_type": "code",
        "outputId": "a7c53fd5-05ca-45ab-d3ce-036b32473c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import os\n",
        "graph_dir = \"graphs/demo\"\n",
        "os.makedirs(graph_dir)\n",
        "with tf.Graph().as_default() as g:\n",
        "    const_a = tf.constant(1., shape=[1, 5], name=\"const_a\")\n",
        "    const_b = tf.add(const_a, 5, name=\"const_b\")\n",
        "    var_c = tf.get_variable(\"var_c\", shape=[5, 3])\n",
        "    const_d = tf.matmul(const_b, var_c, name=\"const_d\")\n",
        "    # create a writer \n",
        "    writer = tf.summary.FileWriter(graph_dir, tf.get_default_graph())\n",
        "#To run TensorBoard, use the following command: tensorboard --logdir=path/to/log-directory\n",
        "#where logdir points to the directory where the FileWriter serialized its data.\n",
        "#If this logdir directory contains subdirectories which contain serialized data from separate runs, \n",
        "#then TensorBoard will visualize the data from all of those runs. \n",
        "#Once TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 10:33:57.170203 139872212289408 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HfLEnkBkLgJ",
        "colab_type": "text"
      },
      "source": [
        "## Save and Restore checkpoints\n",
        "\n",
        "Training a deep learning model may take a few hours or even a few days. The learned weights should be saved periodically so that you can restore for further applications. In TensorFlow, all trainable variables can be saved in checkpoints - a binary file that map variable names to tensor values.\n",
        "\n",
        "TensorFlow provides a superb class tf.train.Saver to do this work. Its constructor adds save and restore ops to the graph for all, or a specified list, of the variables in the graph. The Saver object provides methods to run these ops, specifying paths for the checkpoint files to write to or read from.\n",
        "\n",
        "### Save variables\n",
        "\n",
        "Create a Saver with tf.train.Saver() to manage all variables in the model. The following cell shows how to call the  tf.train.Saver.save method to save variables to checkpoint files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M6sofLkimzV",
        "colab_type": "code",
        "outputId": "6adad243-bd83-4ecd-fe38-7bc9d235a529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "ckpt_dir = \"checkpoints/demo\"\n",
        "os.makedirs(ckpt_dir)\n",
        "with tf.Graph().as_default() as g:\n",
        "    const_a = tf.constant(2, tf.int32, [5])\n",
        "    var_b = tf.get_variable(\"var_b\", dtype=tf.int32, shape=[5],\n",
        "                            initializer=tf.zeros_initializer) # variable\n",
        "    const_c = var_b + const_a\n",
        "    var_d = tf.get_variable(\"var_d\", shape=[3],\n",
        "                            initializer=tf.ones_initializer) # variable\n",
        "    \n",
        "    print(\"[Graph]\", const_a, var_b, const_c, var_d, sep=\"\\n\")\n",
        "    print(\"\\n[Trainable variables]\", *tf.trainable_variables(), sep=\"\\n\")\n",
        "    \n",
        "    init_op = tf.global_variables_initializer()\n",
        "    # Declare a saver object to save checkpoints\n",
        "    saver = tf.train.Saver() \n",
        "\n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Initialize variables\n",
        "    sess.run(init_op)\n",
        "    \n",
        "    # Do some works with the model\n",
        "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
        "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")\n",
        "    \n",
        "    # Save the variables to disk\n",
        "    save_path = saver.save(sess, os.path.join(ckpt_dir, \"model.ckpt\"))\n",
        "    print(\"\\n[Model saved in path: {}]\".format(save_path))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Graph]\n",
            "Tensor(\"Const:0\", shape=(5,), dtype=int32)\n",
            "<tf.Variable 'var_b:0' shape=(5,) dtype=int32_ref>\n",
            "Tensor(\"add:0\", shape=(5,), dtype=int32)\n",
            "<tf.Variable 'var_d:0' shape=(3,) dtype=float32_ref>\n",
            "\n",
            "[Trainable variables]\n",
            "<tf.Variable 'var_b:0' shape=(5,) dtype=int32_ref>\n",
            "<tf.Variable 'var_d:0' shape=(3,) dtype=float32_ref>\n",
            "\n",
            "[Value]\n",
            "[2 2 2 2 2]\n",
            "[0 0 0 0 0]\n",
            "[2 2 2 2 2]\n",
            "[1. 1. 1.]\n",
            "\n",
            "[Model saved in path: checkpoints/demo/model.ckpt]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duygom-0kz3w",
        "colab_type": "text"
      },
      "source": [
        "### Inspect variables in checkpoints\n",
        "This is a useful function to debug. Sometimes you may notice that you have some conflict between your model and checkpoints. That's why you need this function to insepct what you stored in the checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3IXQbZ0kxqQ",
        "colab_type": "code",
        "outputId": "7140631a-1a25-469d-fccc-104d9c2249b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
        "# tensor_name: Name of the tensor in the checkpoint file to print. \n",
        "# all_tensors: Boolean indicating whether to print all tensors.\n",
        "# all_tensor_names: Boolean indicating whether to print all tensor names.\n",
        "print_tensors_in_checkpoint_file(save_path,\n",
        "                                 tensor_name=\"\",\n",
        "                                 all_tensors=\"\",\n",
        "                                 all_tensor_names=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "var_b (DT_INT32) [5]\n",
            "var_d (DT_FLOAT) [3]\n",
            "\n",
            "# Total number of params: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTKoCgRDk9pA",
        "colab_type": "text"
      },
      "source": [
        "### Restore variables\n",
        "**The tf.train.Saver object not only saves variables to checkpoint files, it also restores variables.**Note that when you restore variables you do not have to initialize them beforehand. For example, the following snippet demonstrates how to call the tf.train.Saver.restore method to restore variables from the checkpoint files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GVufCQLlEd3",
        "colab_type": "code",
        "outputId": "2ea0025b-dbad-494b-b6f0-a14db8b26f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "with tf.Session(graph=g) as sess:\n",
        "    saver.restore(sess, save_path)#saver = tf.train.Saver() \n",
        "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
        "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 10:34:02.921810 139872212289408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Value]\n",
            "[2 2 2 2 2]\n",
            "[0 0 0 0 0]\n",
            "[2 2 2 2 2]\n",
            "[1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekG6yAxlLG3",
        "colab_type": "code",
        "outputId": "c3c1167c-5ef8-4c2b-c8db-f77134d3df34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# more robust\n",
        "with tf.Session(graph=g) as sess:\n",
        "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "    print(ckpt)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
        "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"checkpoints/demo/model.ckpt\"\n",
            "all_model_checkpoint_paths: \"checkpoints/demo/model.ckpt\"\n",
            "\n",
            "\n",
            "[Value]\n",
            "[2 2 2 2 2]\n",
            "[0 0 0 0 0]\n",
            "[2 2 2 2 2]\n",
            "[1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBVMI-JlNdh",
        "colab_type": "text"
      },
      "source": [
        "# Importing data\n",
        "## Dataset and Iterator\n",
        "The tf.data API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. **The tf.data API makes it easy to deal with large amounts of data, different data formats, and complicated transformations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opunP9QylXRv",
        "colab_type": "text"
      },
      "source": [
        "The tf.data API introduces two new abstractions to TensorFlow:\n",
        "\n",
        "\n",
        "*   A **tf.data.Dataset** represents **a sequence of elements**, in which each element contains one or more Tensor objects. For example, in an image pipeline, an element might be a single training example, with a pair of tensors representing the image data and a label. There are two distinct ways to create a datas\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "> *   Creating a **source** (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more **tf.Tensor** objects.\n",
        "*  Applying a **transformation** (e.g. Dataset.batch()) constructs a dataset from one or more **tf.data.Dataset** objects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   A **tf.data.Iterator provides the main way to extract elements from a dataset.** **The operation returned by Iterator.get_next() yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model.** The simplest iterator is a \"one-shot iterator\", which is associated with a particular Dataset and iterates through it once. For more sophisticated uses, the Iterator.initializer operation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2MzleF1l0T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()#Ê∏ÖÁ©∫defaultÁöÑgraphË∑ünodes\n",
        "## Pseudo dataset\n",
        "dataset_size = 20\n",
        "# 20 examples, each example has 5 features\n",
        "data = np.random.rand(dataset_size, 5)\n",
        "# this dataset has 3 labels\n",
        "label = np.random.randint(low=0, high=3, size=dataset_size)  #return [0,3)‰πãÈñìÁöÑÈö®Ê©üÊï¥Êï∏ÔºålowË∑ühighÂèØ‰ª•ÁúÅÁï•‰∏çÂØ´"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMsb4awwl2jR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "05ab8d35-ba84-432f-defc-890b90e64391"
      },
      "source": [
        "batch_size = 7\n",
        "\n",
        "## Create a `dataset` by `from_tensor_slices`\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
        "print(\"[Original dataset] \\n\", training_dataset)\n",
        "training_dataset = training_dataset.batch(batch_size)\n",
        "print(\"\\n[Transformed dataset] \\n\", training_dataset)\n",
        "\n",
        "## Create a `iterator` to extract elements from `dataset`\n",
        "training_iterator = training_dataset.make_initializable_iterator()\n",
        "x_input, y_label = training_iterator.get_next()\n",
        "print(\"\\n[Iterator] \\n\", training_iterator)\n",
        "print(\"\\n[Elements extracted by iterator] \\n\", x_input, \"\\n\", y_label)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 10:34:07.701331 139872212289408 deprecation.py:323] From <ipython-input-19-79014ffe8365>:10: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Original dataset] \n",
            " <DatasetV1Adapter shapes: ((5,), ()), types: (tf.float64, tf.int64)>\n",
            "\n",
            "[Transformed dataset] \n",
            " <DatasetV1Adapter shapes: ((?, 5), (?,)), types: (tf.float64, tf.int64)>\n",
            "\n",
            "[Iterator] \n",
            " <tensorflow.python.data.ops.iterator_ops.Iterator object at 0x7f36303c3208>\n",
            "\n",
            "[Elements extracted by iterator] \n",
            " Tensor(\"IteratorGetNext:0\", shape=(?, 5), dtype=float64) \n",
            " Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3paOFsml4qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "2147b348-f105-4da4-b6f8-4ed658ba64be"
      },
      "source": [
        "config = tf.ConfigProto() #tf.ConfigProto‰∏ÄËà¨Áî®Âú®ÂàõÂª∫sessionÁöÑÊó∂ÂÄô„ÄÇÁî®Êù•ÂØπsessionËøõË°åÂèÇÊï∞ÈÖçÁΩÆ\n",
        "config.gpu_options.allow_growth = True # avoids occupying full memory of GPU\n",
        "# ‰ΩøÁî®allow_growth optionÔºåÂàö‰∏ÄÂºÄÂßãÂàÜÈÖçÂ∞ëÈáèÁöÑGPUÂÆπÈáèÔºåÁÑ∂ÂêéÊåâÈúÄÊÖ¢ÊÖ¢ÁöÑÂ¢ûÂä†ÔºåÁî±‰∫é‰∏ç‰ºöÈáäÊîæ\n",
        "#ÂÜÖÂ≠òÔºåÊâÄ‰ª•‰ºöÂØºËá¥Á¢éÁâá\n",
        "with tf.Session(config=config) as sess:\n",
        "#with ËØ≠Âè•ÈÄÇÁî®‰∫éÂØπËµÑÊ∫êËøõË°åËÆøÈóÆÁöÑÂú∫ÂêàÔºåÁ°Æ‰øù‰∏çÁÆ°‰ΩøÁî®ËøáÁ®ã‰∏≠ÊòØÂê¶ÂèëÁîüÂºÇÂ∏∏ÈÉΩ‰ºöÊâßË°åÂøÖË¶ÅÁöÑ‚ÄúÊ∏ÖÁêÜ‚ÄùÊìç‰ΩúÔºåÈáäÊîæËµÑÊ∫êÔºåÊØîÂ¶ÇÊñá‰ª∂‰ΩøÁî®ÂêéËá™Âä®ÂÖ≥Èó≠ÔºèÁ∫øÁ®ã‰∏≠ÈîÅÁöÑËá™Âä®Ëé∑ÂèñÂíåÈáäÊîæÁ≠â„ÄÇ\n",
        "    sess.run(training_iterator.initializer) # initialize the iterator \n",
        "    step = 0 # record the steps\n",
        "    try:\n",
        "        while(True):\n",
        "            x_, y_ = sess.run([x_input, y_label])\n",
        "            step += 1\n",
        "            print(\"{} batch - {} examples\".format(step, len(y_)))\n",
        "            print(x_, y_)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 batch - 7 examples\n",
            "[[0.41081383 0.06597323 0.71284072 0.84300783 0.08216232]\n",
            " [0.66156032 0.48323296 0.20343833 0.40993505 0.69330441]\n",
            " [0.38975034 0.91370855 0.27921714 0.60641643 0.86260992]\n",
            " [0.16886322 0.58602098 0.38523981 0.67848347 0.54274835]\n",
            " [0.47789346 0.94618161 0.25224474 0.27373856 0.9394098 ]\n",
            " [0.76813638 0.30714627 0.67788903 0.76350365 0.78466302]\n",
            " [0.30735643 0.78984279 0.05231301 0.74166844 0.30045917]] [2 1 0 2 0 1 0]\n",
            "2 batch - 7 examples\n",
            "[[0.01375201 0.11260345 0.1776381  0.78408999 0.10636911]\n",
            " [0.3041357  0.45466305 0.39321503 0.62538437 0.46070151]\n",
            " [0.8107471  0.09974872 0.52445138 0.33798521 0.59092114]\n",
            " [0.562637   0.43672363 0.68434467 0.84051257 0.63860076]\n",
            " [0.59173833 0.1606597  0.01657958 0.4911257  0.49616353]\n",
            " [0.88308619 0.68672709 0.3924523  0.39702625 0.9076844 ]\n",
            " [0.91431348 0.86509187 0.50354021 0.94254006 0.59251835]] [2 0 1 2 2 0 2]\n",
            "3 batch - 6 examples\n",
            "[[0.85888602 0.59701074 0.04038532 0.52777364 0.72303408]\n",
            " [0.18629104 0.93228522 0.99538796 0.07643483 0.18157548]\n",
            " [0.07695708 0.42123248 0.05142309 0.04778688 0.58618526]\n",
            " [0.04940355 0.27654584 0.3918795  0.73098794 0.2375072 ]\n",
            " [0.16075528 0.14325217 0.89368278 0.85882628 0.06615262]\n",
            " [0.89375519 0.21583837 0.9536946  0.90131766 0.55088619]] [0 2 1 1 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ymNb_93mAzG",
        "colab_type": "text"
      },
      "source": [
        "## Build basic model\n",
        "We have learned essential concepts in TensorFlow. In this part, we will introduce two methods to build a basic model which has one fully connected layer. There are many ways to build a neural network. Here we guide you to use low level and high level method to construct it. Although high level method provides compact utilities in tf.layers, it is better to understand the concepts of deep learning model using low level method in the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAwyezelmGco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting \n",
        "feature_dims = 784 # example with 784 features\n",
        "neurons = 1024 # fully connected layer with 1024 neurons\n",
        "classes = 10 # 10 classes classification problem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id7pb5ElmIto",
        "colab_type": "text"
      },
      "source": [
        "### Low level\n",
        "\n",
        "\n",
        "*   Construct the layers in neural network from scratch.\n",
        "*   Define weights and bias as trainable variables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH-JWFsVmQdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fully_connected_layer(x_inputs, out_dim, name='fc'):\n",
        "    \"\"\" Low level method\n",
        "        x_inputs: a batch examples [batch_size, feature_dims]\n",
        "        out_dim: neurons in this layer.\n",
        "    \"\"\" \n",
        "    in_dim = x_inputs.shape[-1] # feature_dims\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        weights = tf.get_variable(\"weights\", shape=[in_dim, out_dim])\n",
        "        bias = tf.get_variable(\"bias\", shape=[out_dim])\n",
        "        out = tf.matmul(x_inputs, weights) + bias\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqkPgsxwmR2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "03f83d6c-a4ff-4090-8099-db8b3e9fe0ce"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32, shape=[None, feature_dims])\n",
        "fc = fully_connected_layer(x, neurons, \"fc\")\n",
        "out = fully_connected_layer(fc, classes, \"logits\")\n",
        "print(\"[Output tensor]\", fc, out, sep=\"\\n\")\n",
        "print(\"\\n[Variables] \", *tf.trainable_variables(), sep=\"\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Output tensor]\n",
            "Tensor(\"fc/add:0\", shape=(?, 1024), dtype=float32)\n",
            "Tensor(\"logits/add:0\", shape=(?, 10), dtype=float32)\n",
            "\n",
            "[Variables] \n",
            "<tf.Variable 'fc/weights:0' shape=(784, 1024) dtype=float32_ref>\n",
            "<tf.Variable 'fc/bias:0' shape=(1024,) dtype=float32_ref>\n",
            "<tf.Variable 'logits/weights:0' shape=(1024, 10) dtype=float32_ref>\n",
            "<tf.Variable 'logits/bias:0' shape=(10,) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYxDhlcSmUxr",
        "colab_type": "text"
      },
      "source": [
        "### High level\n",
        "\n",
        "\n",
        "*   Construct the layers in neural network using tf.layers.\n",
        "\n",
        "*   A high level API provided by TensorFlow.\n",
        "*   It contains a lot of useful methods and arguments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02kPQ7zCmf8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "66519ff8-a846-4a2d-da5f-0ebcd58f612d"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32, shape=[None, feature_dims])\n",
        "fc = tf.layers.dense(x, neurons, activation=tf.nn.relu, name=\"fc\")\n",
        "out = tf.layers.dense(fc, classes, \n",
        "                      activation=tf.nn.softmax, name=\"logits\")\n",
        "print(\"[Output tensor]\", fc, out, sep=\"\\n\")\n",
        "print(\"\\n[Variables] \", *tf.trainable_variables(), sep=\"\\n\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 10:34:15.997817 139872212289408 deprecation.py:323] From <ipython-input-24-1a705e637879>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Output tensor]\n",
            "Tensor(\"fc/Relu:0\", shape=(?, 1024), dtype=float32)\n",
            "Tensor(\"logits/Softmax:0\", shape=(?, 10), dtype=float32)\n",
            "\n",
            "[Variables] \n",
            "<tf.Variable 'fc/kernel:0' shape=(784, 1024) dtype=float32_ref>\n",
            "<tf.Variable 'fc/bias:0' shape=(1024,) dtype=float32_ref>\n",
            "<tf.Variable 'logits/kernel:0' shape=(1024, 10) dtype=float32_ref>\n",
            "<tf.Variable 'logits/bias:0' shape=(10,) dtype=float32_ref>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nmsHefKmjRo",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning Model Walkthrough**\n",
        "\n",
        "\n",
        "1.   Build a **Graph** that:\n",
        "\n",
        "\n",
        "*   Define **dataset** and **iterator**\n",
        "\n",
        "*   Build the **model**\n",
        "\n",
        "*   Define the **loss**\n",
        "*   Define the **optimizer**\n",
        "\n",
        "\n",
        "*   Other tensors or operations you need (optional)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.   Execute a **session** to:\n",
        "\n",
        "\n",
        "*   Initialize the variables\n",
        "*  Run the target tensors and operations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQDfzSGgnLho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## template\n",
        "with tf.Graph().as_default() as g:\n",
        "    \"\"\" Define dataset and iterator \"\"\"\n",
        "    with tf.name_scope(\"data\"):\n",
        "        pass\n",
        "    \n",
        "    \"\"\" Build the model \"\"\"\n",
        "    with tf.name_scope(\"model\"):\n",
        "        pass\n",
        "\n",
        "    \"\"\" Define the loss \"\"\"\n",
        "    with tf.name_scope(\"loss\"):\n",
        "        pass\n",
        "    \n",
        "    \"\"\" Define the optimizer \"\"\"\n",
        "    with tf.name_scope(\"optimizer\"):\n",
        "        pass\n",
        "    \n",
        "    \"\"\" Other tensors or operations you need \"\"\"\n",
        "    with tf.name_scope(\"accuracy\"):\n",
        "        pass\n",
        "\n",
        "with tf.Session(graph=g) as sess:\n",
        "    \"\"\" Initialize the variables \"\"\"\n",
        "    \"\"\" Run the target tensors and operations \"\"\"\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mblKvlsrKf8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "ac83911e-e4f8-4964-bc4d-72f67df78dd8"
      },
      "source": [
        "#tf.variable_scopeÂèØ‰ª•ËÆ©ÂèòÈáèÊúâÁõ∏ÂêåÁöÑÂëΩÂêçÔºåÂåÖÊã¨tf.get_variableÂæóÂà∞ÁöÑÂèòÈáèÔºåËøòÊúâtf.VariableÁöÑÂèòÈáè\n",
        "#tf.name_scopeÂèØ‰ª•ËÆ©ÂèòÈáèÊúâÁõ∏ÂêåÁöÑÂëΩÂêçÔºåÂè™ÊòØÈôê‰∫étf.VariableÁöÑÂèòÈáè\n",
        "#example:\n",
        "import tensorflow as tf;  \n",
        "import numpy as np;  \n",
        "import matplotlib.pyplot as plt;  \n",
        " \n",
        "with tf.variable_scope('V1'):\n",
        "\ta1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
        "\ta2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
        "with tf.variable_scope('V2'):\n",
        "\ta3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
        "\ta4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
        "  \n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.initialize_all_variables())\n",
        "\tprint(a1.name)\n",
        "\tprint(a2.name)\n",
        "\tprint(a3.name)\n",
        "\tprint(a4.name)\n",
        "print('--------------------- ')\n",
        "import tensorflow as tf;  \n",
        "import numpy as np;  \n",
        "import matplotlib.pyplot as plt;  \n",
        " \n",
        "with tf.name_scope('V1'):\n",
        "\ta1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
        "\ta2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
        "with tf.name_scope('V2'):\n",
        "\ta3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n",
        "\ta4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')\n",
        "  \n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.initialize_all_variables())\n",
        "\tprint(a1.name)\n",
        "\tprint(a2.name)\n",
        "\tprint(a3.name)\n",
        "\tprint(a4.name)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0727 06:43:59.152180 139855175137152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "V1/a1:0\n",
            "V1/a2:0\n",
            "V2/a1:0\n",
            "V2/a2:0\n",
            "--------------------- \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-90e6e7baca3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'V2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0ma3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0ma4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable a1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-33-90e6e7baca3c>\", line 24, in <module>\n    a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqX417uenOb1",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec\n",
        "Word2Vec is a computationally-efficient model that learns to embed words into vectors. The goal is to map words that have similar meanings close to each other.\n",
        "\n",
        "## Why represent words as vectors?\n",
        "When dealing with words, a straightforward way would be treating each word as discrete symbols. For instance, cat as 2 and dog as 1. However, these symbols carry no information about the original word, making it impossible for us to infer the relationship between cats and dogs (both are four-legged animals and both are pets) based on the symbols alone. Hence, to successfully learn the relationship between them, we might need a large amount of training data.\n",
        "\n",
        "On the other hand, **Vector space models (VSMs)** which represent words as vectors can help overcome these obstacles. This is based on a key observation that **semantically similar words are often used interchangeably in different contexts.** For example, the words cat and dog may both appear in a context \"___ is my favorate pet.\" When feeding cat and dog into the NN to predict their nearby words, these two words will be likely to share the same/similar hidden representation in order to predict the same/similar nearby words.\n",
        "## Skip-Gram and CBOW\n",
        "Word2Vec comes in two variants **Skip-Gram and CBOW (Continuous Bag-Of-Words).** Algorithmically, these models are similar. **CBOW predicts the target words using its neighborhood(context) whereas Skip-Gram does the inverse, which is to predict context words from the target words.** For example, given the sentence the quick brown fox jumped over the lazy dog. Defining the context words as the word to the left and right of the target word, CBOW will be trained on the dataset:\n",
        "\n",
        "([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)...\n",
        "\n",
        "where CBOW tries to predict the target word quick from the context words in brackets [the, brown], and predict brown from [quick, fox] and so on. However, with Skip-Gram, the dataset becomes\n",
        "\n",
        "(quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
        "\n",
        "where Skip-Gram predicts the context word the, brown with the target word quick. Statistically, **CBOW** smoothes over a lot of the distributional information (by treating an entire context as one example). For the most part, this turns out to be a **useful thing for smaller datasets**. On the other hand, **Skip-Gram** **treats each context-target pair as a new observation and is shown to be able to capture the semantics better when we have a large dataset**\n",
        "Skip-GramÊòØÁªôÂÆöinput wordÊù•È¢ÑÊµã‰∏ä‰∏ãÊñá„ÄÇËÄåCBOWÊòØÁªôÂÆö‰∏ä‰∏ãÊñáÔºåÊù•È¢ÑÊµãinput word„ÄÇ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1NPuyMeQ3ST",
        "colab_type": "text"
      },
      "source": [
        "Note that the tasks described above are only used to train the neural network, we don‚Äôt use the neural network for the task we trained it on. What we want is the weights of the hidden layer, the \"embedding matrix\".\n",
        "\n",
        "For the rest of the tutorial, we will focus on the Skip-Gram model.\n",
        "\n",
        "## Noise Constrative Estimation\n",
        "Before we start implementing a skip-gram model, we want to introduce an important techniques that **reduce the computing efforts**. It is called **noise constrative estimation**.\n",
        "\n",
        "Let's motivate this idea by a naive method. If we want to create a skip-gram model, we can use the following snippets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxJme03KPGRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "2c963f7f-5841-456d-b007-07d524512349"
      },
      "source": [
        "# Naive idea, this is not runnable.\n",
        "\n",
        "vocabulary_size = 10000\n",
        "embedding_size = 128\n",
        "batch_size = 64\n",
        "\n",
        "with tf.Graph().as_default() as g:\n",
        "    center_words = tf.placeholder(tf.int32, [batch_size])\n",
        "    target_words = tf.placeholder(tf.int32, [batch_size])\n",
        "    \n",
        "    encode_matrix = tf.get_variable(\"encoder\",\n",
        "                                    shape=[vocabulary_size, embedding_size])\n",
        "    decode_matrix = tf.get_variable(\"decoder\",\n",
        "                                    shape=[embedding_size, vocabulary_size])\n",
        "    \n",
        "    embedding = tf.matmul(center_words, encode_matrix)\n",
        "    logits = tf.matmul(embedding, decode_matrix)\n",
        "\n",
        "    output = tf.nn.softmax(logits)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    528\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m   1019\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"encoder/read:0\", shape=(10000, 128), dtype=float32)'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-8e7971ecb56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                     shape=[embedding_size, vocabulary_size])\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2647\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5923\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5924\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5925\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5926\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    561\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 563\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Input 'b' of 'MatMul' Op has type float32 that does not match type int32 of argument 'a'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc36kzY3RMkM",
        "colab_type": "text"
      },
      "source": [
        "As we can see, it has a large number of parameters in the network and the softmax layer is a computationally intensive task.\n",
        "\n",
        "Is there any solution to solve it?\n",
        "\n",
        "**NCE** comes to rescue the problem of softmax.\n",
        "\n",
        "Like most neural networks, a Skip-Gram model is trained using the maximum likelihood(ML) principle:\n",
        "\n",
        "**argmin‚àë=‚àílogP(y(i)|x(i),Œò)**\n",
        "\n",
        "In a multiclass task where y=1,‚ãØ,V(V being the vocabulary size) we usually assume\n",
        "\n",
        "**Pr(y|x)‚àºCategorical(y|x;œÅ)**\n",
        "\n",
        "It is natural to use **V Softmax units** in the output layer. That is, the activation a(L)i of each unit at the last layer(layer L) z(L)i outputs one dimension of the softmax function, a generalization of the logistic sigmoid\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RymF_pCrSO-n",
        "colab_type": "text"
      },
      "source": [
        "On the other hand, **for feature learning in word2vec we do not need a full probabilistic model**. **The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression)** to discriminate the real target words wt from k imaginary (noise) words wÃÉ , in the same context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmueG5llSeI2",
        "colab_type": "text"
      },
      "source": [
        "Since we are sampling from two distributions, the correct word is sampled from the true distribution P according to the context c and noise words are sampled from Q, which is a noise distribution, in practice, it is said to be a uniform distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np5p_l_ISpLM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1353c6a-b9b7-4191-cec2-700e8812531f"
      },
      "source": [
        "# Download the data.\n",
        "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
        "DATA_FOLDER = \"data\"\n",
        "FILE_NAME = \"text8.zip\"\n",
        "EXPECTED_BYTES = 31344016\n",
        "\n",
        "def make_dir(path):\n",
        "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    \n",
        "def download(file_name, expected_bytes):\n",
        "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
        "    local_file_path = os.path.join(DATA_FOLDER, file_name)\n",
        "    if os.path.exists(local_file_path):\n",
        "        print(\"Dataset ready\")\n",
        "        return local_file_path\n",
        "    file_name, _ = urllib.request.urlretrieve(\n",
        "      os.path.join(DOWNLOAD_URL, file_name), local_file_path)\n",
        "    file_stat = os.stat(local_file_path)\n",
        "    if file_stat.st_size == expected_bytes:\n",
        "        print('Successfully downloaded the file', file_name)\n",
        "    else:\n",
        "        raise Exception(\n",
        "              'File ' + file_name +\n",
        "              ' might be corrupted. You should try downloading it with a browser.')\n",
        "    return local_file_path    \n",
        "    \n",
        "make_dir(DATA_FOLDER)\n",
        "file_path = download(FILE_NAME, EXPECTED_BYTES)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded the file data/text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uswiCDzQSpsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d312954-3572-438f-b877-d3a00ce1b6a7"
      },
      "source": [
        "# Read the data into a list of strings.\n",
        "def read_data(file_path):\n",
        "    \"\"\" Read data into a list of tokens\"\"\"\n",
        "    with zipfile.ZipFile(file_path) as f:\n",
        "        # tf.compat.as_str() converts the input into the string\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "vocabulary = read_data(file_path)\n",
        "print('Data size', len(vocabulary))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 17005207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZU1cZuhTK1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the dictionary\n",
        "import collections\n",
        "# Build the dictionary and replace rare words with UNK token.\n",
        "def build_dataset(words, n_words):\n",
        "    \"\"\" Create two dictionaries and count of occuring words\n",
        "        - word_to_id: map of words to their codes\n",
        "        - id_to_word: maps codes to words (inverse word_to_id)\n",
        "        - count: map of words to count of occurrences\n",
        "    \"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    word_to_id = dict() # (word, id)\n",
        "    # record word id\n",
        "    for word, _ in count:\n",
        "        word_to_id[word] = len(word_to_id)\n",
        "    id_to_word = dict(zip(word_to_id.values(), word_to_id.keys())) # (id, word)\n",
        "    return word_to_id, id_to_word, count\n",
        "\n",
        "def convert_words_to_id(words, dictionary, count):\n",
        "    \"\"\" Replace each word in the dataset with its index in the dictionary\"\"\"\n",
        "    data_w2id = []\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        index = dictionary.get(word, 0)\n",
        "        if index == 0:\n",
        "            unk_count += 1\n",
        "        data_w2id.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    return data_w2id, count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju2jbWiNTNpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Filling 4 global variables:\n",
        "# data_w2id - list of codes (integers from 0 to vocabulary_size-1).\n",
        "              This is the original text but words are replaced by their codes\n",
        "# count - map of words(strings) to count of occurrences\n",
        "# word_to_id - map of words(strings) to their codes(integers)\n",
        "# id_to_word - maps codes(integers) to words(strings)\n",
        "\"\"\"\n",
        "\n",
        "vocabulary_size = 50000\n",
        "word_to_id, id_to_word, count = build_dataset(vocabulary, vocabulary_size)\n",
        "data_w2id, count = convert_words_to_id(vocabulary, word_to_id, count)\n",
        "del vocabulary  # reduce memory."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI3QISwXTQRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d289b5f-2d8b-4440-f44a-704be8eb0adf"
      },
      "source": [
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data: {}'.format(data_w2id[:10]))\n",
        "print([id_to_word[i] for i in data_w2id[:10]])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
            "Sample data: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuy2Uh20TS_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility function\n",
        "def generate_sample(center_words, context_window_size):\n",
        "    \"\"\" Form training pairs according to the skip-gram model.\"\"\"\n",
        "    for idx, center in enumerate(center_words):\n",
        "        context = random.randint(1, context_window_size)\n",
        "        # get a random target before the center word\n",
        "        for target in center_words[max(0, idx - context) : idx]:\n",
        "            yield center, target\n",
        "        # get a random target after the center word\n",
        "        for target in center_words[idx + 1 : idx + context + 1]:\n",
        "            yield center, target\n",
        "\n",
        "def batch_generator(data, skip_window, batch_size):\n",
        "    \"\"\" Group a numeric stream into batches and yield them as Numpy arrays.\"\"\"\n",
        "    single_gen = generate_sample(data, skip_window)\n",
        "    while True:\n",
        "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
        "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
        "        for idx in range(batch_size):\n",
        "            center_batch[idx], target_batch[idx] = next(single_gen)\n",
        "        yield center_batch, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-wfuDHTV_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Skip-gram word2vec model\n",
        "## some training settings\n",
        "training_steps = 1000\n",
        "skip_step = 100\n",
        "graph_dir = \"graphs/word2vec_simple\"\n",
        "ckpt_dir = \"checkpoints/word2vec_simple\"\n",
        "\n",
        "## some hyperparameters\n",
        "batch_size = 128\n",
        "embed_size = 128\n",
        "num_sampled = 64\n",
        "learning_rates = 1.0\n",
        "\n",
        "## geneartor for `tf.data.Dataset`\n",
        "def gen():\n",
        "    \"\"\" Return a python generator that generates batches. \"\"\"\n",
        "    yield from batch_generator(data_w2id, 2, batch_size)\n",
        "\n",
        "## model\n",
        "def word2vec(dataset):\n",
        "    \n",
        "    \"\"\" 1. Build the graph\"\"\" \n",
        "    with tf.name_scope(\"data\"):\n",
        "        # one_shot_iterator doesn't need to be initialized\n",
        "        iterator = dataset.make_one_shot_iterator() \n",
        "        # get the input and output\n",
        "        center_words, target_words = iterator.get_next() \n",
        "    \n",
        "    with tf.name_scope('embed'):\n",
        "        embedding_matrix = tf.get_variable(\"embedding_matrix\",\n",
        "                                           shape=[vocabulary_size, embed_size])\n",
        "        embedding = tf.nn.embedding_lookup(embedding_matrix,\n",
        "                                           center_words, name='embedding')\n",
        "        \n",
        "    with tf.name_scope('loss'):\n",
        "        initializer = tf.truncated_normal_initializer(stddev=1.0 / (embed_size ** 0.5))\n",
        "        nce_weight = tf.get_variable('nce_weight',\n",
        "                                     shape=[vocabulary_size, embed_size],\n",
        "                                     initializer=initializer)\n",
        "        nce_bias = tf.get_variable('nce_bias', shape=[vocabulary_size],\n",
        "                                   initializer=tf.zeros_initializer)\n",
        "\n",
        "        # define loss function to be NCE loss function\n",
        "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
        "                                            biases=nce_bias, \n",
        "                                            labels=target_words, \n",
        "                                            inputs=embedding, \n",
        "                                            num_sampled=num_sampled, \n",
        "                                            num_classes=vocabulary_size), name='loss')\n",
        "    with tf.name_scope('optimizer'):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rates).minimize(loss)\n",
        "    \n",
        "    ## store checkpoints\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    \"\"\" 2. Execute a session \"\"\"\n",
        "    config = tf.ConfigProto() \n",
        "    config.gpu_options.allow_growth = True  # avoids occupying full memory of GPU\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "\n",
        "        # if that checkpoint exists, restore from checkpoint\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "        # we use this to calculate late average loss in the last SKIP_STEP steps\n",
        "        total_loss = 0.0 \n",
        "        writer = tf.summary.FileWriter(graph_dir, sess.graph)\n",
        "\n",
        "        for index in range(1, training_steps+1):\n",
        "            try:\n",
        "                loss_batch, _ = sess.run([loss, optimizer])\n",
        "                total_loss += loss_batch\n",
        "                if index % skip_step == 0:\n",
        "                    print('Average loss at step {}: {:5.1f}'.format(\n",
        "                        index, total_loss / skip_step))\n",
        "                    total_loss = 0.0\n",
        "                    saver.save(sess,\n",
        "                               os.path.join(ckpt_dir, \"model\"),\n",
        "                               index)\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                pass\n",
        "        writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeaIpAgTb-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "6b493a4f-21a4-4086-c25f-2dd6a398e900"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32),\n",
        "                                         (tf.TensorShape([batch_size]),\n",
        "                                          tf.TensorShape([batch_size, 1])))\n",
        "word2vec(dataset)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 10:36:41.349372 139872212289408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0729 10:36:41.373674 139872212289408 deprecation.py:323] From <ipython-input-33-54bb20ec224c>:23: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "W0729 10:36:41.617611 139872212289408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step 100: 232.7\n",
            "Average loss at step 200: 192.0\n",
            "Average loss at step 300: 164.1\n",
            "Average loss at step 400: 151.9\n",
            "Average loss at step 500: 139.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0729 10:36:43.784431 139872212289408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss at step 600: 130.5\n",
            "Average loss at step 700: 120.5\n",
            "Average loss at step 800: 116.6\n",
            "Average loss at step 900: 109.8\n",
            "Average loss at step 1000: 103.3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}