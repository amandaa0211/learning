{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "吳_W12-1(未完全理解)_Nuts and Bolts of Convolution Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f-j-C1ZZaaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "07abf4c5-272a-49ad-d6cf-a03220ae4dee"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "dest_directory = 'dataset/mnist'\n",
        "# check the directory\n",
        "if not os.path.exists(dest_directory):\n",
        "  os.makedirs(dest_directory)\n",
        "# import data\n",
        "mnist = input_data.read_data_sets(\"dataset/mnist/\", one_hot=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 11:26:03.901258 140266438244224 deprecation.py:323] From <ipython-input-3-26b791a563db>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0729 11:26:03.903437 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0729 11:26:03.905310 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting dataset/mnist/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0729 11:26:04.196214 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0729 11:26:04.199192 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0729 11:26:04.258478 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting dataset/mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting dataset/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBrcoDjZe9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Softmax Regression on MNIST\n",
        "#Before jumping to Convolutional Neural Network model, \n",
        "#we're going to start with a very simple model with a single layer and softmax regression.\n",
        "def fully_connected_layer(x_inputs, out_dim, name='fc'):\n",
        "  \"\"\" \n",
        "      x_inputs: a batch examples [batch_size, feature_dims]\n",
        "      out_dim: neurons in this layer.\n",
        "  \"\"\" \n",
        "  in_dim = x_inputs.shape[-1] # feature_dims\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "      weights = tf.get_variable(\"weights\", shape=[in_dim, out_dim])\n",
        "      bias = tf.get_variable(\"bias\", shape=[out_dim])\n",
        "      out = tf.matmul(x_inputs, weights) + bias\n",
        "      return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FPp3B6GZubE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_model_fully(object):\n",
        "  def __init__(self, dataset, hps):\n",
        "    self.dataset = dataset\n",
        "    #setting hyperparameters\n",
        "    self.training_steps = hps.steps\n",
        "    self.batch_size = hps.steps\n",
        "    self.input_dim = hps.input_dim\n",
        "    self.output_dim = hps.output_dim\n",
        "    #define weights and build model architecture\n",
        "    self._build_model()\n",
        "    \n",
        "  def _build_model(self):\n",
        "    with tf.name_scope(\"input\") as scope:\n",
        "      self.x = tf.placeholder(tf.float32,[None, self.input_dim])  # flatten into vector of 28 x 28 = 784\n",
        "      self.y_true = tf.placeholder(tf.float32, [None, self.output_dim])  # true answers\n",
        "\n",
        "    with tf.name_scope(\"network\") as scope:\n",
        "      self.y_pred = fully_connected_layer(self.x, self.output_dim, name='out')\n",
        "\n",
        "    with tf.name_scope(\"loss_func\") as scope:\n",
        "      self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_true, \n",
        "                                                                                  logits=self.y_pred))\n",
        "      correct_prediction = tf.equal(tf.argmax(self.y_pred, 1), tf.argmax(self.y_true, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    with tf.name_scope(\"train_op\") as scope:\n",
        "      self.train_op = tf.train.GradientDescentOptimizer(0.5).minimize(self.cross_entropy)  \n",
        "      \n",
        "  def train(self):\n",
        "    for step in range(self.training_steps):\n",
        "      batch_x, batch_y = self.dataset.train.next_batch(self.batch_size)\n",
        "      self.sess.run(self.train_op, feed_dict = {\n",
        "          self.x: batch_x,\n",
        "          self.y_true: batch_y\n",
        "        })\n",
        "  def test(self):\n",
        "    test_x, test_y = self.dataset.test.images, self.dataset.test.labels\n",
        "    test_accuracy = self.sess.run(self.accuracy, feed_dict = {\n",
        "          self.x: test_x, \n",
        "          self.y_true: test_y\n",
        "      })\n",
        "    print(\"Testing Accuracy : %.3f\" %test_accuracy)\n",
        "  def run(self):\n",
        "    #define session and initialize variables\n",
        "    self.sess = tf.Session()\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "    self.train()\n",
        "    self.test()\n",
        "    self.sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnuUXJAyZwFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_hps= tf.contrib.training.HParams(\n",
        "  steps = 1000, \n",
        "  batch_size = 32,\n",
        "  input_dim = 784,\n",
        "  output_dim = 10\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDqcnwKZy5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "4864a608-d320-459c-cf17-1ada43c0db6e"
      },
      "source": [
        "mnist_fc_model = MNIST_model_fully(mnist, model_hps)\n",
        "mnist_fc_model.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 11:26:16.836151 140266438244224 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0729 11:26:16.860781 140266438244224 deprecation.py:323] From <ipython-input-5-fe6b573c73c2>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy : 0.923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9EV0L1pZ1wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Multilayer Convolutional Network on MNIST\n",
        "# Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input.\n",
        "# Our pooling is plain old max pooling over 2x2 blocks.\n",
        "def conv_block(x, name, kernel_width, kernel_height, inp_channel, out_channel, strides = [1, 1, 1, 1], padding='SAME'):\n",
        "  W_conv = tf.get_variable(name+'w', [kernel_width, kernel_height, inp_channel, out_channel])\n",
        "  b_conv = tf.get_variable(name+'b', [out_channel])\n",
        "  return tf.nn.relu(tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)+b_conv)\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(\n",
        "      x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwm2kr8DaL24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_model_cnn(object):\n",
        "  def __init__(self, dataset, hps):\n",
        "    self.dataset = dataset\n",
        "    #setting hyperparameters\n",
        "    self.training_steps = hps.steps\n",
        "    self.batch_size = hps.steps\n",
        "    self.image_size = hps.image_size\n",
        "    self.output_dim = hps.output_dim\n",
        "    self.dropout_keep_prob = hps.dropout_keep_prob\n",
        "    #define weights and build model architecture\n",
        "    self._build_model()\n",
        "    \n",
        "  def _build_model(self):\n",
        "    with tf.name_scope(\"input\") as scope:\n",
        "      self.x = tf.placeholder(tf.float32,[None, self.image_size*self.image_size*1])  # flatten into vector of 28 x 28 = 784\n",
        "      self.y_true = tf.placeholder(tf.float32, [None, self.output_dim])  # true answers\n",
        "\n",
        "    with tf.name_scope(\"network\") as scope:\n",
        "      # self.y_pred = fully_connected_layer(self.x, self.output_dim, name='out')\n",
        "      x_image = tf.reshape(self.x, [-1, self.image_size, self.image_size, 1])\n",
        "      # 1st convolutional layer\n",
        "      # cnn kernel : [5, 5, 1] *32, output shape = [batch, 28, 28, 32]\n",
        "      h_conv1 = conv_block(x_image, \"conv1\", 5, 5, 1, 32)\n",
        "      # 2x2 pooling, output shape = [batch, 14, 14, 32]\n",
        "      h_pool1 = max_pool_2x2(h_conv1)\n",
        "      \n",
        "      # 2nd convolutional layer\n",
        "      h_conv2 = conv_block(h_pool1, \"conv2\", 5, 5, 32, 64)\n",
        "      # 2x2 pooling, output shape = [batch, 7, 7, 64]\n",
        "      h_pool2 = max_pool_2x2(h_conv2)\n",
        "      \n",
        "      # Densely connected layer\n",
        "      # Flatten the feature maps into [batch, 7 x 7 x 64]\n",
        "      h_pool2_flat = tf.reshape(h_pool2, [-1, self.image_size//4 * self.image_size//4 * 64]) \n",
        "      \n",
        "      # We add a fully-connected layer with 1024 neurons to allow processing on the entire image.\n",
        "      h_fc1 = tf.nn.relu(fully_connected_layer(h_pool2_flat, 1024, name='fc1'))\n",
        "      \n",
        "      # Dropout, this can prevent overfitting\n",
        "      self.keep_prob = tf.placeholder(tf.float32)\n",
        "      h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
        "      \n",
        "      # Finally, we add the output layer, just like for the one layer softmax regression above.\n",
        "      self.y_pred = fully_connected_layer(h_fc1_drop, self.output_dim, name='out')\n",
        "      \n",
        "    # After defining our model, we then define our loss and optimizer.\n",
        "    with tf.name_scope(\"loss_func\") as scope:\n",
        "      self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_true, \n",
        "                                                                                  logits=self.y_pred))\n",
        "      correct_prediction = tf.equal(tf.argmax(self.y_pred, 1), tf.argmax(self.y_true, 1))\n",
        "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    with tf.name_scope(\"train_op\") as scope:\n",
        "      self.train_op = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
        "      \n",
        "  def train(self):\n",
        "    for step in range(self.training_steps):\n",
        "      batch_x, batch_y = self.dataset.train.next_batch(self.batch_size)\n",
        "      _, accuracy = self.sess.run((self.train_op, self.accuracy), feed_dict = {\n",
        "          self.x: batch_x,\n",
        "          self.y_true: batch_y,\n",
        "          self.keep_prob: self.dropout_keep_prob\n",
        "        })\n",
        "      if step % 200 == 0:\n",
        "        print(\"Step %d, Training Accuracy: %.3f\" %(step, accuracy))\n",
        "        \n",
        "    print(\"Step %d, Training Accuracy: %.3f\" %(step, accuracy))\n",
        "        \n",
        "  def test(self):\n",
        "    test_x, test_y = self.dataset.test.images, self.dataset.test.labels\n",
        "    test_accuracy = self.sess.run(self.accuracy, feed_dict = {\n",
        "          self.x: test_x, \n",
        "          self.y_true: test_y, \n",
        "          self.keep_prob: 1.0 #when testing, we don't apply dropout\n",
        "      })\n",
        "    print(\"Testing Accuracy : %.3f\" %test_accuracy)\n",
        "  def run(self):\n",
        "    #define session and initialize variables\n",
        "    self.sess = tf.Session()\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "    self.train()\n",
        "    self.test()\n",
        "    self.sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DnE5RYvbczi",
        "colab_type": "text"
      },
      "source": [
        "To reduce overfitting, we will apply dropout before the readout layer. The idea behind dropout is to train an ensemble of model instead of a single model. During training, we drop out neurons with probability p, i.e., the probability to keep is 1−p. When a neuron is dropped, its output is set to zero. These dropped neurons do not contribute to the training phase in forward pass and backward pass. For each training phase, we train the network slightly different from the previous one. It's just like we train different networks in each training phrase. However, during testing phase, we don't drop any neuron, and thus, implement dropout is kind of like doing ensemble. Also, randomly drop units in training phase can prevent units from co-adapting too much. Thus, dropout is a powerful regularization techique to deal with overfitting.\n",
        "\n",
        "We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyuCWtTranrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_hps_cnn = tf.contrib.training.HParams(\n",
        "  steps = 2000, \n",
        "  batch_size = 32,\n",
        "  image_size = 28,\n",
        "  output_dim = 10,\n",
        "  dropout_keep_prob = 0.5\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWsBHxQYbhBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7aa2f8a8-907c-4520-8af0-e0d1e7c9e33c"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "CNN_model = MNIST_model_cnn(mnist, model_hps_cnn)\n",
        "CNN_model.run()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 11:26:29.824797 140266438244224 deprecation.py:506] From <ipython-input-9-fc444b1cdba9>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVbAkHEFbwjV",
        "colab_type": "text"
      },
      "source": [
        "Before jumping to a complicated neural network model, we're going to start with **KNN** and **SVM.** The motivation here is to compare neural network model with traditional classifiers, and highlight the performance of neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilWLovRDbyR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "95a05324-850c-426b-e28d-164b7866ace1"
      },
      "source": [
        "# Loading Data\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# convert class vectors to binary vectors\n",
        "Y_train = np_utils.to_categorical(y_train)\n",
        "Y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('Y_train shape:', Y_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('Y_test shape:', Y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "X_train shape: (50000, 32, 32, 3)\n",
            "Y_train shape: (50000, 10)\n",
            "X_test shape: (10000, 32, 32, 3)\n",
            "Y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoAg7OkKb8CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preprocessing\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af7D3wH3cIiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "43184881-398c-4c36-becb-269b52b84900"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# transform an 3-channel image into one channel\n",
        "def grayscale(data, dtype='float32'):\n",
        "  # luma coding weighted average in video systems\n",
        "  r = np.asarray(.3, dtype=dtype)\n",
        "  g = np.asarray(.59, dtype=dtype)\n",
        "  b = np.asarray(.11, dtype=dtype)\n",
        "  rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
        "  # add channel dimension\n",
        "  rst = np.expand_dims(rst, axis=3)\n",
        "  return rst\n",
        "\n",
        "X_train_gray = grayscale(X_train)\n",
        "X_test_gray = grayscale(X_test)\n",
        "\n",
        "# plot a randomly chosen image\n",
        "img = round(np.random.rand() * X_train.shape[0])\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X_train[img], interpolation='none')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(\n",
        "    X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAACICAYAAAABDZUdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztfVuMXMd55vf36evcZzgXDu+kREmm\npUhKBMcLyWsjXhtyEER5CIL4YeEFDPhlF0iAfbCRtwAJ4Lxk9yUvAmzED0EcYxM4QjaAYBg2dgMk\nXjOOLhEl8SaJIoczQw7nPn3v2ofurv+r8pxhk5xpzoj/BwiqPnPqVNWpZvX338U5B4PBYACAzMOe\ngMFg2D+wA8FgMHjYgWAwGDzsQDAYDB52IBgMBg87EAwGg4cdCAaDweOBDgQReVlE3heRyyLyrd2a\nlGF/wfb50YHcr2OSiCQALgL4EoDrAH4O4KvOuQu7Nz3Dw4bt86OF7AP0/QyAy865qwAgIt8H8AqA\n1C9KPim4Uq4EACjklJwMlMJpODhq03U6vESw7T2VcjN4Vnmr4dstulGgD8gQTyoOJEF/oac3GtrO\n57VTJiPbtgGg1dJ2o6kfErqvkNcxHXWv1cK1NGkBTZpLvabPzdN7jY96Gh7ZzivfqJRRqVcF6bjn\nfc5msy6Xy7Xnk8/766VSKbgv7cco3Oftp1Yul4PPW1tbd+2foY0uFotBf76v0dDvDM+f+3MbAFq0\n0c1mc9v7CoXCdktBrVYLPnN/bvN9PK/4PfJckqT93apUKqjX6zvtM4AHOxCOAviYPl8H8Os7dSjl\nSnjpxBcAAGeO6oY89+mJ4L5qS19CI6OLq1e1nctpu0H/it5/ZzV41ptv3PbtWk03h/9Blgb0/qd+\nZSzon4iOc3tJN+T0Ce00MKivsVjSjQKA8pauZWldv8RjJT0ETp445Nsu0c396Fq4lrUtHf/Okn5p\nb17f9O1Ts0O+XW+F+7++qX3GDrXn/A9v/BR3wT3vcy6Xw5kzZwAAJ06c8NefeeaZ4D7+h8eoVCq+\nzV98/qJfuBCeR2+88YZv8z8c/gc5MKB79vTTTwf9+b7bt/U7w/MfGtJ3Gx9ufCCtruq+DQ4O+vap\nU6d8mw+gDz/8MHjW5qbu5507d3z7+vXrvn3s2DHf5vcCABsbG749MdH+t3X+/Hn0ggc5EHqCiHwD\nwDcAoJgt3eVuw0EF73OXHRgOHh7kQLgB4Dh9Pta5FsA59yqAVwFgND/qGpX26T04rKf1mxeXgz7n\nnhjx7VJRf0nffPOWbw8O6/Uzp0foekjl6nR4VukkdU5/uesVooLRGxkmEWJtTa/fvqO/CIVlvWdg\nKKJ/ROcOTemBOFLUgdbX9Rfh1i3tnyuE4stQSf+hVelsbRFzqlT1V9ch/OXIQZ9X6qwrFnG2wT3v\nc7FYdNVqtT1n+lWNf9Wfeuop32YmwL/23L/LOuLrQEit05gHo0ulu+Bfcv6F51/olZUV3x4eHg76\n86/05OSkbzOT4OcyC+G1x3NhtlSv17e9HoPZTnf8WMRJ7dvTXdvj5wDOishpEckD+H0Arz3A8wz7\nE7bPjxDumyE45xoi8t8AvA4gAfBd59w7uzYzw76A7fOjhQfSITjn/hHAP/Y8WDbB1GSb6pW3lEp/\nOLcW3Hf8sGpjJwtKuTY3lTL94j0VHy59oP2PH1a6BQADY0qJ78ypUi9L2vhWTeeysa5jAMDiQtW3\nK5v6rPK6UrYRelazGNK/2WO6lml62w2WLBpKN69c1LWUBsNnzRzRtU2P6ZjDz6pS9gNSMDYqIQEc\nGSNaO9GmzNnkriLDfexzFlNTUwBCBdnHH38c3HfkyBGdD9Hs9fV133777bd9+8qVK9v2BUIKv0ay\nHeszumIMECreAGB+ft632YLB97FlItaTHD161LezWd1oFl+4ffHiRd9mEQEI19ZVCgLA888/79vX\nrl3z7dhKMTo66tuHDh36pTntBPNUNBgMHnYgGAwGDzsQDAaDx577ITDqzRYWVtrmOnbCmxkLvcau\nz6l5Zo5k+Aw57RQGdOrXbqqcl4vshuWK6gSa0HYpp2bPhDwA566HsmW9qhPNJ6oPcORYMjBEsn4p\nlC2bNJ/5BZWnh3OqG6nSkNW6msOKjdBs6Br6Lgo0ppA827qpfVY2QtlybFzNsyuL7XfB3pe7hUaj\n4c1qbCpjPQEQOtqwDM/yLpvtbty4se09QGiGY8+9NJ8IHhsI9Qvchx2I2NQZP5fvW1hY8G3WO7Dz\nEr8XNpkCoa6B9QtsKuXxWGcCAGNj6lzX3YdeTLGAMQSDwUCwA8FgMHj0VWQYKCX4lXNtOjM4oJRn\nY7ka3NciD7uF28qnCxQEdWZWqdTFilLEO6uhB9f6htKxJNHzr8qBIhQLsabOaACAs8fUhNOigKit\nuo555LBS8fVqSM3evawP/PVzSpmzUMo5Tx5wgyQ9HZ2iIAsAoyP6x/kVfWfvXFZvuo11Hb+QDcWB\nRlVp6mKH1TbqoViyGyiVSj5ugeMH2NMPCL37FhcXfZtp9vHj6iTJZsfl5dC7lU2VTK1ZFGBvvXgu\nHGfAIgf3n52d9e04uOrSpUu+/eyzz247Js+ZRaHDhw8HzxofH/dt9pR8//33fTttvUBohuyKYiyi\n7ARjCAaDwcMOBIPB4NFXkaGQT3D2dNujjIODVtdDmj00pJTtmU8pfSqRmHGNPPIuXFSrxEY1pMmJ\n0yUWiL61WtvnVhgshq+Eg6tqFJadEAMbGiTxJwo531hT+lYgardF149Os5VFrQe5JFxLtaIUe35e\nKevcgranx1XEubYY0uJMkTTbHe/MenP3rQyFQgGPPfYYAGBpaclfZ5oLhBr0c+fO+TaLGezd+N57\n7/l27J3HWne2QLBYwvfE4cssprBGnkUGtjLwdSDdO5Kvz8zM+DaLEjHlZ4sJW19u3rzp22yxmZub\nC/pzsFR3LbElIw3GEAwGg4cdCAaDwaOvIsPWVgPn/7VNIeuOKF8Uk7+2rpTnsSd0iscOKx2+eVPp\nrzSVcq2vhdRoME9OO6B0VJQPQURp8/EjUfamplLDdcp+VK+RmEBLYUcoADhxWGnmwqKKSWeOqSg0\nOaIOT4srSqs3t8JnLa9q/wxZTBpk8cglSpEzLtzeGuVNKFfa62LRabewubnpM/Qw/Y5j8lmEeOKJ\nJ3ybg3uYMrP2P3bGYcrP98XZhLpg60U8Tw5oSstBEFsZOLiJHZNOnjzp2+wwxKIUOywB4XtJS+3G\nYlGcZo7Fqe48095DDGMIBoPBww4Eg8Hg0VeRwcHBdTT1J44qlS4OhOfS8jJlSm6pOLC6rvR9YFBp\n4TjFNVTXQwrM2lV2eGo5StJKIkc+El+2GuTMQjkQhilXwcKqjrF4O9R+ZxP9/DGJOc99Winm0Lhq\nvN+9rg465a1wLm9dVWtKhWIQyiQKLCwrlZ6dDDXppw6Tw8tmm5Ze2dqb34QuRWVqHmc6ZucgprQs\nDrDFgbX8nGcBCPc5TWTg67H4wnScKTuPyU5Ct25pPg4gpO0cJ8GJZdnh6OrVq74dix/sgJQmsvD4\n3dwTXXAC1m7atjj/QxqMIRgMBg87EAwGg0dfRYZckmCqE4I7XFAqV8qFjhmTp8hJhLT5b72lmtmr\nc0rryhTLUMyGYanlFoXFcukSYuPZrJ6LtXrocDK3pNS0XFNaOXVILQMf3VQqf2sp7D9MBV2QKK19\n95KKBl/5smYfTjK69kwmtDKsllX8WC+TNYEsDmsbOseZ0GCCJpQ2thrtudxv5a6dkM1mfeouFhNi\nkYGzKDM1fuutt3ybU4Wl1WuI+6cVamGHodi3n2MpeJzuOoBQFGArARCujcURTpX2la98xbfTQpmB\nUBxiCwT3YREgFhnuNQM1wxiCwWDwsAPBYDB42IFgMBg8+qpDqNYauPph23STzatJbPJQWAVn6rDK\nY7cW1SRz7YbKTVeu6fUS6QbGh0PZskXFX7dqJGfSWTgyoONVo7RlKxR4JRycRDngVsijsBx5hB2m\nWo+OPCVvLepa8iRDHp5UM9dalAKtkKXxM9vL/o06eTCG3XHkiOZtaLm2nMo5InYL1WoVH3zwAQDg\n7Nmz/nqcQo3zALAMzwFNXPeQZeiREV0LEJoK48CjLtiEGOsQ2NTJOoC01O1xf06XzjoM9lrk+XOg\nUxz0xbqOtIpLPH68XjY7bjf2Trjrt0FEvisiiyLy73RtQkR+JCKXOv8f3+kZhv0P22cD0JvI8JcA\nXo6ufQvAj51zZwH8uPPZcLDxl7B9fuRxV5HBOfd/RORUdPkVAF/otL8H4KcAvnm3Z4mIr5hUIJo+\nMhbS/EJOTXprW1zJSGlPk6j92Lg+a6KkfQFguUyZinNUUaelZ2GxyCagkFo1G0rnK2TBuXpLRZbh\nEX1urhauZYTKvh8+RF5vqzovoWzQubyKFaOjYUWfyREVsyr0XppktSILLDKt0OQ0PaxUNHe0/ezC\npfZ72PV97gTfsKchB/cAoemQ6Tj3YbMZe/rFxV65Pz+XTXBsGoxNfTwOezdyrgEWU+J8DDxnNgOy\nN2ZaNmiutASE64wDn9LGZ/B77q6TvR93wv0KkDPOuW62hnkAMzvdbDiwsH1+xPDAGiXXPvZSvVtE\n5Bsicl5Ezlca2yt7DPsf97LP9+oMY9g/uF8rw4KIzDrnborILIDFtBudc68CeBUApoYm3EiHtjZr\nSsuq1ZCm3yD6Nzevh8jKGtM/7VMnWri8FX4Z8+QpOFFQKrmyqfdxCjREdT1yeZ1nhdKNtZw+6+gU\nFXTNh1aGySH92+Expfxzt9UbjYwfyJMlYXldPSABYHZGKe/SkooMtYb2OTKlCzh9JKToR0bUmjM6\n2l5/Pr+j9vm+9nlkZMR1i68ytY214UzHuc0a/7TUZnGwDosJnB6NNfhM62OtO1P4NDrOloFYZGGa\nztYUtp6wZYDH6wYgbTdOt9AKEK5/enrat0+cOBH0Z+/KrjgSe3am4X4ZwmsAvtZpfw3A39/ncwz7\nG7bPjxh6MTv+NYB/BvCkiFwXka8D+DaAL4nIJQD/qfPZcIBh+2wAerMyfDXlT1+818FyOcHsbJsC\nVitKyy59eCe4r9lQEeDGvAaaLCwpZcolSvObTYqlXw/pXjGvS0yEMjDnKDcCjdcqhDkETpxVaraw\npBrjLiUGgMdOaJ9fPRylYNvU+RyfVm1yg8ScSk1FkQkSK24th2nChobJmkGiBUSf9fyn1Skl2wyL\n1rTImjEx0abP3cCu3d3nnE+DxoFCnAMACCkwZxRmmsypwthiEOdDKBRUNGNnHqbKbD2IazOyAxXT\nfLYsnD592rc5MAsIrQHscMVrZFGELQm8XiD8bsU1LLvgYjBxejS2ZnTFh7TnxDDXZYPB4GEHgsFg\n8OhrLEM2Kzg00aZqS6Sv/mgupMY5UvVfuqxUrETWgCJRwRrFFbioVmGN06aRmFEaVspdIQee8eEw\nZv/zLz7n26tE7apUD7K28JFvnzkexqZX6jq3YzOqiX55WEWDJpcMr+oZfXgidExarylNzueVFhYo\nzdyVy5pa65nHwv5c6jHfKVMvCB10dgPZbNb79nOqr7gEOzsHcd1Gtgak5TCIC4+kpUpjawDT99hK\n8LnPfc63mcLzOHyda0ECoTjANSCZ/vNaOG1anM8gLe8D9+dakpyxOkavokIXxhAMBoOHHQgGg8HD\nDgSDweDRVx1CRjIodWSi6VkdevBQKMe+877KegnJ/aNDlC49r9cLFJB05nTokZUpqDy5Sla4GlU1\n4gKxlagi0MSQnplPzaqp6bX//S++fYhMhfnRME7/vQuUqy+j+pDZCdUnVJq63uvzKotOToV5IlbJ\nPDlDgU4TJV3L3IrKvE+f1QpIADB9VOe2sdV51u6nVISIeA9Dlqfj4CYOuGHPQZbvWW5meTiWu/k+\nNgGyDoDl9jgHAZsXudrSD3/4w23nz7oBAHj77bd9m3UY7LXIOhDWp8Rr4XmmBXRxTsdPfepTQX+u\nIhWbZ+8GYwgGg8HDDgSDweDRV5EBcMh0KiYNU7Wl4kA4jYlBpVaf/6wGepTJu7FGZqbfekmp/K+d\nDKlcjahhhRIHrG0orZxf1WddWQhFhuPHlCbWqHJUrqHBNTOj6s24vB7Gr8/Na9DO8JBSwyblUHAk\n8qxv0hj5kM9PTyhl/O3Pq2ddoah9ltZ0jdNHwzj7CqWbrzbbtNTthcxAGBxU02echp1p94svvujb\nTPnZnPjFL6rTZGxqYxGAxQQOlFpeXvbt2ATKFaaYZvP4nCYtDkji4Cym9kz5GTxGHHjEYsaXv/xl\n32ZvTB6fRYQYXVNrr+n2jSEYDAYPOxAMBoNHnys3ZTA51qaNDUpNtlIOM9g+85RSydkZpb2Xr2oQ\n1NId1cw//R810GN6MoxzL89p1ts8abwdUbanqSLTCythnH3ylBbr/JfX/8m3HzupVHCEqPByVNHn\niSdUe12kgKqtmtLiBuVpmJzQdpIJczuUE/08Ma30UShoa/yY0lVXCN/F8pqKFplC5/3vvqMistms\np8pM32PNPmvH2Rpx+fJl32Zt+ksvveTbnA8ACAOSWGvPGnvW8rP4AITBSq+//rpvc0ATiz9xQBKL\nMEzteXy2knDOgjizMns9xhaILrrBY8AvB2qxONH9W5wyLg3GEAwGg4cdCAaDwaOvIkOSZDDRcaip\nVZW+jR+KNLFZKo7i9MyanFbKtkXpuBqDqpWVk6FmPT+oz86eVlrXWFMqmtlQijW2EloJLn8879u3\nr2sBkWlyQBoc0Dm6TPhKm1AHopES3UeRRgW6fuakWjWypbhwrdLvDL0/oXwQSUH7DIxqkBAAzEzr\nOIm033GxGOWM2wUkSeKdeDhtGmvpgfQiJCwOcH+2UnCaMSCk6VyohFOtsfUidpLi4jBsgeA5s8gQ\nU3DW4vN9vEZO7caiSGx9YdEmLRs0WybiojWcj6E7z3iMNBhDMBgMHnYgGAwGjz6LDILhkTZFzeaU\nPiW5cBpLK0rnlzeUPrUaHOeuVHf5lmp8K4+HtHR9iyjbvIoGhRHS8lIOhK2594L+sqj08VcpnZpQ\nCrPSoM4lyYaafXYHGSX6Wc8o/c9S5uMSWRxyxZDyg/I2uAa3lVY3WzqiROILqI5jo9ymz/n87n8F\nkiTxTkesTY+14WxBSHMsYicftiTslMKMczCw81PaswDgzh21YJ07d8632TLAeRp2yjMQi0Zd8PqZ\nwsd0nsUPFhnSxIdY9OLP3dwKe5112WAwfAJhB4LBYPCwA8FgMHj03ew4Nt42yZRJTozDLoZKKvc1\nmiofl0gGm5lQuXu4oPLUahSctPWBer3Vb1/z7YlPv6A3DajZJjsSynPHnlQTDpoqGzYzOut8Xk2b\nrUbodZkfUfOUK6g5KhGdszSpT05lPWlFsiE/OwhWCfMDdtGI5lKrqwdcgvY6JcX09yBIksTL0WxC\ni8HmOdYbsAmRA33YbBfrAK5d071lfcCTTz65bX8eGwAef/xx304LBEqrIhU/j3UFLM+z3M86iHi8\nOF/k3RC/Y/Z07M45zcQbo5dCLcdF5CcickFE3hGRP+hcnxCRH4nIpc7/tw/rMhwI2D4bgN5EhgaA\n/+6cOwfgswD+q4icA/AtAD92zp0F8OPOZ8PBhe2zoafKTTcB3Oy010XkXQBHAbwC4Aud274H4KcA\nvrnTs5JcgrGp9g9M0ZHIING5RF54hyjdeZaoWI4qLCVTatraLIcpo6ZPqOdiQoVXfXAPAHEkloyH\nHmw1uq9V00CVVln7uISoeGRCbQqZAdlUSEt2RCUlo6KQRMJUK0vecZRtvkUUkdOqZ+LIJRbTusFl\n3Uxqu7jP2WzWB+XsRH+Z6qZVVWLxgc15cWowzmfAogE/i6l57KmYlu6dU6KzdyLPKwavmcfcqXIU\ng+k9908Tv2KvSR5nT/MhiMgpAM8D+BmAmc6XCADmAcyk9PFlwpeiSELD/sSD7vPKysp2txgOAHo+\nEERkCMDfAvhD51yguXPt42fbI8g596pz7gXn3AuHxrZXfhn2D3Zjn+NfX8PBQU9WBhHJof0l+Svn\n3N91Li+IyKxz7qaIzAJYTH9C9zkZZDtazxHKshsp05GfVk17o8o0SalRMqqWiHqOMjNHnne5RO/L\n5EnjK0rfM02l3K1qWCy2SanWmlUVGWprynYaOf03UhwMA03KtykenpjdAIk5GUptJnX699YMKaIj\n8QNUoNZVdAyXIa9JF1LJDD17fbW9Li6Uu1v7nMlkvHab6Wys6WZqz0FMDPYu5MzMMeXe6W9dMJVn\nTTwQ5i3guXBwVFpmaCD0uuR1sqcm9+e5xMVa0+bM89rJarBdDoqdxmD0YmUQAN8B8K5z7s/pT68B\n+Fqn/TUAf9/TiIZ9CdtnA9AbQ3gRwH8G8LaIvNG59kcAvg3gByLydQAfAfi9vZmioU+wfTb0ZGX4\nJ6Qn2vpiyvV0tNqPcoFjUiiWOtK0I0NaftamspNNlhxBYrpIxV9rFRITRMVjDhTidGQAkM2qNjmh\n8RMKFmlVKEtwI6S+uaKSsBytJZNlawIVKqWCrhLRPEfjuzoVs6Gsza0WFaElDTkAJCSaFAbb68x0\n5rTb+7wdRY2deZhCp1Fg7rOTWMA0uRKte7s5xZr5NGciDgpiyh6vha0O3J/nzGOyyBJbAPg+HifN\n+hCLW9sFZO2aY5LBYHh0YAeCwWDw6Gssg0CIuijlycQigyNqR1SOnX4yRI1BeRIcQirHdLxBNDub\nUa2ysPhRCH3cZVg14QKqM0gafFdWuthshtrrTE4pI1N2R2exY00/iT+uFlJfR7EJDZozMjq+q5Fl\nohE6BbXIGazRaPfv1WHlXtGlyjs9n//GlJ1pNoPFgl4dnpgqc584PwDnOkib804iQ9r8mf7z+Ezr\n42fxZxYtuA+vMX4XLBp177NCLQaD4Z5hB4LBYPDoq8jg4Dw9bpGYIKWQvrkiiQZMvzLksOSIZlE7\ng4hu0pHXbCj9yomGsmYp5LhVD2k6Uz6huTi2RpCDTcaFPu5JlsKZKW2ao+IwHMvAFhYXr8Xp3Dgs\nvFknrXjCGu6YVtN7jr3BdhldGstUNfb/Z9rONDvNgYeftVPhEabTHLKcFq8ApFsGGDz/WORgOs9z\n6yX+IKbzafEL3Oa1xO9iO2uEiQwGg+GeYQeCwWDwsAPBYDB49FWHAAiaHRk38JyKcgg0qEJRge8j\nkThKOkXN0EOOg3cyTZLbyaTncipnumZoAmqR3kHIvJcZJJmR5ihRQFEgKzY5cEnnnNBqqpSOvBEF\nWmWgn5tkUsxx0BOZWWuRPiThee5BkVdGd90sj8epy1k/kJZqjGXftNwCO92XJnfH/dNMlRyAtZPe\ngv/Gz04LKuJ8DnGgFYPfUZpuI9ZTbKfPsGKvBoPhnmEHgsFg8OiryNCCQ61DyVsUdJRzoXmsSeY1\n5Nm8oqa2CtHhbEGXEaUQCKhxlvIhNDYouEmUvrWSiFqRd2GrotQuw96NOb0nmwv711lk4XnROGx2\nzFE25mYrFF84bVpelMrWaV6cmq1WCV9GktF1Jh3RLU7Tthtwznlvu53oc1qmZabs7B3Ipr7YO4/7\nMGVmap4JRKZwn5iO85hM51nkiMWfNNEgLSApzYMyBs+L58LejHEwV5oJtxcYQzAYDB52IBgMBo/+\nWhlaLTQ79KZKWn7JhcVRauSFlx3VYCMuPML9W03yeHMhRcoRNedCqI5C/3NF9g4M6VuS17nVKhR3\nTtaIDKVta9ZCmk4SQEDOW3XygKTQfqFgqHwhKgJK6eQ4n0KgYaegr0wS5YbgeLDavQW93Aucc552\nM/2OPRXTNPtMh7n/TtR6p2ChLuKiqgwWR5iap+VQiDX7vbzHNIsLWzLi8dOKyvK8YrFgu7wL5qlo\nMBjuGXYgGAwGj/5aGVotlDta34aj+nNJGCgiRKfLdyi7MR9fVFuxVSUNcxI5v2SI8vF1LnTCqbwi\nbbGQpj4hhykhyp4hsaTVCKlZlgOy6lW6T+9J2OGGTAmtKBGxMGWmZWZJzKhXKQN0SWP82+PrWpy3\n8uy+h1Kz2fTa/bQcAPHf0mo5pKUdi1OoMYVOc3JKuw70FoSUFnQV/42fxeJLWjbo2DEp7Z1xfxal\nOJdDPP691ok0hmAwGDzsQDAYDB59jmXQDMtBaqlfcsAhCs3xB1QQhWs7Zik3QRJZCSpE03MUi5DL\nch/S2GZC8aVOmn0hOSNHzlDZOjvfhEvJsI97TeciYIcnWm+dKW5IMTmLslTI4YViGTbW1REnH1FU\ndtIZHghTxe02ulrtNB9/vgfoLZZgJ8cipt1szUjLUxCLL9w/zWGqV/rNz0oTeRpBpu9wLWmOUYy1\nNXWsi3MzcHGZbkGZXbMyiEhRRP6fiLzZKRP+x53rp0XkZyJyWUT+RkTyd3uWYf/C9tkA9CYyVAH8\nhnPuWQDPAXhZRD4L4M8A/A/n3OMAlgF8fe+maegDbJ8Ndz8QXBtdDpLr/OcA/AaA/9W5/j0Av7Mn\nMzT0BbbPBqD3Yq8JgH8F8DiAvwBwBcCKcz6Z4XUAR+/6IOe8kC1k7ipvhsEZLZLVS0PqXZanKkh5\nyoNYGqTCm5lIiCczXJVNSySP5ticGMntnN4gEPUopXuDvBM57TsQ5i3gSkxCMl0zkK3pjI6CvrjC\nFOs6GqSDyZE/ZL0cVfShd+51M2Gewt3ZZ2yfy491GEAok3PxVJaJuc33xHI3B/iw3J1makzzAIz7\n8BxZzxGPzzqBtECnnSpHMdJyIKTlj4iDm9JSv/eCnqwMzrmmc+45AMcAfAbAU70OICLfEJHzInL+\nztrm3TsYHhp2a59XV1f3bI6GvcU9mR2dcysAfgLgPwAYEy2EeAzAjZQ+rzrnXnDOvTAxsreabcPu\n4EH3eXR0tE8zNew27ioyiMgUgLpzbkVESgC+hLai6ScAfhfA99FjmXDnnI/dz1F68no9NDtyfoAK\n0d4i0/mEPfrIAy8XnnEt4vxMpapkjqyTaXOgFAaKtITNgGwO5XxuTboeFxrVPhSbBSHTKqeM44QG\nrVZI91pVMlXxPDlVPLHVZpSCjYtdVRqbnTE6Itwu7nOr1fI0dqfU50yhy2WqpEXr4f4sCsQBPWkp\n2tOqLcXefYw0ys1jpAVQAeF8Jm8RAAACfElEQVQ6uU8afY9FDF5nHPi03XN3SsHWFdPSxJgYvegQ\nZgF8ryNfZgD8wDn3DyJyAcD3ReRPAPwbgO/0NKJhv8L22dBTOfi3ADy/zfWraMuZhk8AbJ8NACB7\nVexz28FEbgHYBHC7b4PuP0xif63/pHNuajcfaPsM4IDuc18PBAAQkfPOuRf6Oug+wqOy/kdlnWk4\nqOu34CaDweBhB4LBYPB4GAfCqw9hzP2ER2X9j8o603Ag1993HYLBYNi/MJHBYDB49PVAEJGXReT9\nTmz9t/o5dr8hIsdF5CcicqGTX+APOtcnRORHInKp8//xhz3X3Ybt88Hd576JDB0PuItou8ReB/Bz\nAF91zl3oywT6DBGZBTDrnPuFiAyjHUX4OwD+C4A7zrlvd/6xjDvnvvkQp7qrsH0+2PvcT4bwGQCX\nnXNXnXM1tH3jX+nj+H2Fc+6mc+4XnfY6gHfRDh1+Be28AsAnM7+A7fMB3ud+HghHAXxMn3uOrT/o\nEJFTaLsF/wzAjHPuZudP8wBmHtK09gq2zwd4n02puMcQkSEAfwvgD51za/w315bXzMzzCcAnZZ/7\neSDcAHCcPqfG1n9SICI5tL8kf+Wc+7vO5YWO3NmVPxcf1vz2CLbPbRzIfe7ngfBzAGc7WXzzAH4f\nwGt9HL+vkHZQ/XcAvOuc+3P602to5xUAeswvcMBg+9zGgdznfkc7/iaA/wkgAfBd59yf9m3wPkNE\nXgLwfwG8DfhEi3+Etnz5AwAnAHwE4Pecc3ceyiT3CLbPB3efzVPRYDB4mFLRYDB42IFgMBg87EAw\nGAwediAYDAYPOxAMBoOHHQgGg8HDDgSDweBhB4LBYPD4/yT4dedNPFOAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x144 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BgMiUdfcMJA",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection\n",
        "When coming to object detection, HOG (histogram of oriented gradients) is often extracted as a feature for classification. It first calculates the gradients of each image patch using sobel filter, then use the magnitudes and orientations of derived gradients to form a histogram per patch (a vector). After normalizing these histograms, it concatenates them into one HOG feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMd9FBS4cQ9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The code is credit to: \"http://www.itdadao.com/articles/c15a1243072p0.html\"\n",
        "def getHOGfeat(image,\n",
        "               stride=8,\n",
        "               orientations=8,\n",
        "               pixels_per_cell=(8, 8),\n",
        "               cells_per_block=(2, 2)):\n",
        "  cx, cy = pixels_per_cell\n",
        "  bx, by = cells_per_block\n",
        "  sx, sy, sz = image.shape\n",
        "  n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
        "  n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
        "  n_blocksx = (n_cellsx - bx) + 1\n",
        "  n_blocksy = (n_cellsy - by) + 1\n",
        "  gx = np.zeros((sx, sy), dtype=np.double)\n",
        "  gy = np.zeros((sx, sy), dtype=np.double)\n",
        "  eps = 1e-5\n",
        "  grad = np.zeros((sx, sy, 2), dtype=np.double)\n",
        "  for i in range(1, sx - 1):\n",
        "    for j in range(1, sy - 1):\n",
        "      gx[i, j] = image[i, j - 1] - image[i, j + 1]\n",
        "      gy[i, j] = image[i + 1, j] - image[i - 1, j]\n",
        "      grad[i, j, 0] = np.arctan(gy[i, j] / (gx[i, j] + eps)) * 180 / math.pi\n",
        "      if gx[i, j] < 0:\n",
        "        grad[i, j, 0] += 180\n",
        "      grad[i, j, 0] = (grad[i, j, 0] + 360) % 360\n",
        "      grad[i, j, 1] = np.sqrt(gy[i, j]**2 + gx[i, j]**2)\n",
        "  normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))\n",
        "  for y in range(n_blocksy):\n",
        "    for x in range(n_blocksx):\n",
        "      block = grad[y * stride:y * stride + 16, x * stride:x * stride + 16]\n",
        "      hist_block = np.zeros(32, dtype=np.double)\n",
        "      eps = 1e-5\n",
        "      for k in range(by):\n",
        "        for m in range(bx):\n",
        "          cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]\n",
        "          hist_cell = np.zeros(8, dtype=np.double)\n",
        "          for i in range(cy):\n",
        "            for j in range(cx):\n",
        "              n = int(cell[i, j, 0] / 45)\n",
        "              hist_cell[n] += cell[i, j, 1]\n",
        "          hist_block[(k * bx + m) * orientations:(k * bx + m + 1) * orientations] = hist_cell[:]\n",
        "      normalised_blocks[y, x, :] = hist_block / np.sqrt(\n",
        "          hist_block.sum()**2 + eps)\n",
        "  return normalised_blocks.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARXMPyzocXy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a648fe86-94cf-4709-b5e8-4aaf460fc663"
      },
      "source": [
        "X_train_hog = []\n",
        "X_test_hog = []\n",
        "\n",
        "print('This will take some minutes.')\n",
        "\n",
        "for img in X_train_gray:\n",
        "  img_hog = getHOGfeat(img)\n",
        "  X_train_hog.append(img_hog)\n",
        "\n",
        "for img in X_test_gray:\n",
        "  img_hog = getHOGfeat(img)\n",
        "  X_test_hog.append(img_hog)\n",
        "\n",
        "X_train_hog_array = np.asarray(X_train_hog)\n",
        "X_test_hog_array = np.asarray(X_test_hog)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This will take some minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UYWj2-qcYjV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8dcd8ffa-5f5c-485b-b972-7938f8ccf2b6"
      },
      "source": [
        "#K Nearest Neighbors (KNN) on CIFAR-10\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# p=2 and metric='minkowski' means the Euclidean Distance\n",
        "knn = KNeighborsClassifier(n_neighbors=11, p=2, metric='minkowski')\n",
        "\n",
        "knn.fit(X_train_hog_array, y_train.ravel())\n",
        "y_pred = knn.predict(X_test_hog_array)\n",
        "print('[KNN]')\n",
        "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[KNN]\n",
            "Misclassified samples: 5334\n",
            "Accuracy: 0.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6tHSJItcff6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a31193b1-5adf-4f19-d97f-cf4ecccbda2e"
      },
      "source": [
        "#Support Vector Machine (SVM) on CIFAR-10\n",
        "# SVM\n",
        "from sklearn.svm import SVC \n",
        "\n",
        "# C is the hyperparameter for the error penalty term\n",
        "# gamma is the hyperparameter for the rbf kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=0, gamma=0.2, C=10.0)\n",
        "\n",
        "svm_linear.fit(X_train_hog_array, y_train.ravel())\n",
        "y_pred = svm_linear.predict(X_test_hog_array)\n",
        "print('[Linear SVC]')\n",
        "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test.ravel(), y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Linear SVC]\n",
            "Misclassified samples: 4940\n",
            "Accuracy: 0.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO5n-azfcyRL",
        "colab_type": "text"
      },
      "source": [
        "# Input Pipeline\n",
        "## Queues\n",
        "Because tf.Session objects are designed to be **multithreaded** and thread-safe, so multiple threads can easily use the same session and run ops in parallel. Queues are useful because of the ability to **compute tensor asynchronously** in a graph. Most of the time, we use queues to handle inputs. In this way, multiple threads prepare training example and enequeue these examples. In addition, only parts of inputs would be read into memory a time, instead of all of them. This can **avoid out of memory error when data is large.**\n",
        "## Typical Input Pipeline\n",
        "1. The list of filenames\n",
        "2. Optional filename shuffling\n",
        "3. Optional epoch limit\n",
        "4. Filename queue\n",
        "5. A Reader for the file format\n",
        "6. A decoder for a record read by the reader\n",
        "7. Optional preprocessing\n",
        "8. Example queue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ3mo6EvdW2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from six.moves import urllib\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56QIgWv7dbWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Data Manually\n",
        "# the url to download CIFAR-10 dataset (binary version)\n",
        "# see format and details here: http://www.cs.toronto.edu/~kriz/cifar.html\n",
        "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "DEST_DIRECTORY = 'dataset/cifar10'\n",
        "# the image size we want to keep\n",
        "IMAGE_HEIGHT = 32\n",
        "IMAGE_WIDTH = 32\n",
        "IMAGE_DEPTH = 3\n",
        "IMAGE_SIZE_CROPPED = 24\n",
        "BATCH_SIZE = 128\n",
        "# Global constants describing the CIFAR-10 data set.\n",
        "NUM_CLASSES = 10 \n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
        "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "366vv18-dfbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2e4a94c9-581d-4ee8-da95-c120b7843efc"
      },
      "source": [
        "def maybe_download_and_extract(dest_directory, url):\n",
        "  \"\"\"\n",
        "    Download the dataset and extract the data\n",
        "  \"\"\"\n",
        "    \n",
        "  if not os.path.exists(dest_directory):\n",
        "    os.makedirs(dest_directory)\n",
        "  file_name = 'cifar-10-binary.tar.gz'\n",
        "  file_path = os.path.join(dest_directory, file_name)\n",
        "  # if have not downloaded yet\n",
        "  if not os.path.exists(file_path):\n",
        "    def _progress(count, block_size, total_size):\n",
        "      sys.stdout.write('\\r%.1f%%' % \n",
        "            (float(count * block_size) / float(total_size) * 100.0))\n",
        "      sys.stdout.flush()  # flush the buffer\n",
        "\n",
        "    print('>> Downloading %s ...' % file_name)\n",
        "    file_path, _ = urllib.request.urlretrieve(url, file_path, _progress)\n",
        "    file_size = os.stat(file_path).st_size\n",
        "    print('\\r>> Total %d bytes' % file_size)\n",
        "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
        "  if not os.path.exists(extracted_dir_path):\n",
        "    # Open for reading with gzip compression, then extract all\n",
        "    tarfile.open(file_path, 'r:gz').extractall(dest_directory)\n",
        "  print('>> Done')\n",
        "\n",
        "# download it\n",
        "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading cifar-10-binary.tar.gz ...\n",
            ">> Total 170052171 bytes\n",
            ">> Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU38EBOtdpqR",
        "colab_type": "text"
      },
      "source": [
        "After downloading the dataset, we create functions\n",
        "\n",
        "* distort_input(training_file, batch_size) to get a training example queue.\n",
        "* eval_input(testing_file, batch_size) to get a testing example queue.\n",
        "* read_cifar10(filename_queue) to read a record from dataset with a filename queue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FroP9DV8dmhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the folder store the dataset\n",
        "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
        "# (1) a list of training/testing filenames\n",
        "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1,6)]\n",
        "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7fn5susdz7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (5) + (6)\n",
        "def read_cifar10(filename_queue):\n",
        "  \"\"\" Reads and parses examples from CIFAR10 data files.\n",
        "    -----\n",
        "    Args:\n",
        "        filename_queue: \n",
        "            A queue of strings with the filenames to read from.\n",
        "    Returns:\n",
        "        An object representing a single example, with the following fields:\n",
        "        height: \n",
        "            number of rows in the result (32)\n",
        "        width: \n",
        "            number of columns in the result (32)\n",
        "        depth: \n",
        "            number of color channels in the result (3)\n",
        "        key: \n",
        "            a scalar string Tensor describing the filename & record number for this example.\n",
        "        label: \n",
        "            an int32 Tensor with the label in the range 0..9.\n",
        "        image: \n",
        "            a [height, width, depth] uint8 Tensor with the image data\n",
        "  \"\"\"\n",
        "\n",
        "  class CIFAR10Record(object):\n",
        "    pass\n",
        "\n",
        "  result = CIFAR10Record()\n",
        "  # CIFAR10 consists of 60000 32x32 'color' images in 10 classes\n",
        "  label_bytes = 1  # 10 class\n",
        "  result.height = IMAGE_HEIGHT\n",
        "  result.width = IMAGE_WIDTH\n",
        "  result.depth = IMAGE_DEPTH\n",
        "  image_bytes = result.height * result.width * result.depth\n",
        "  # bytes of a record: label(1 byte) followed by pixels(3072 bytes)\n",
        "  record_bytes = label_bytes + image_bytes\n",
        "  # (5) reader for cifar10 file format\n",
        "  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
        "  # read a record\n",
        "  result.key, record_string = reader.read(filename_queue)\n",
        "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
        "  # (6) decoder\n",
        "  record_uint8 = tf.decode_raw(record_string, tf.uint8)\n",
        "  # get the label and cast it to int32\n",
        "  result.label = tf.cast(\n",
        "      tf.strided_slice(record_uint8, [0], [label_bytes]), tf.int32)\n",
        "  # [depth, height, width], uint8\n",
        "  depth_major = tf.reshape(\n",
        "      tf.strided_slice(record_uint8, [label_bytes],\n",
        "                       [label_bytes + image_bytes]),\n",
        "      [result.depth, result.height, result.width])\n",
        "  # change to [height, width, depth], uint8\n",
        "  result.image = tf.transpose(depth_major, [1, 2, 0])\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3azUkWjjd68a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distort_input(training_files, batch_size):\n",
        "  \"\"\" Construct distorted input for CIFAR training using the Reader ops.\n",
        "    -----\n",
        "    Args:\n",
        "        training_files: \n",
        "            an array of paths of the training files.\n",
        "        batch_size: \n",
        "            Number of images per batch.\n",
        "    Returns:\n",
        "        images: Images. \n",
        "            4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
        "        labels: Labels. \n",
        "            1D tensor of [batch_size] size.\n",
        "  \"\"\"\n",
        "  for f in training_files:\n",
        "    if not tf.gfile.Exists(f):\n",
        "      raise ValueError('Failed to find file: ' + f)\n",
        "  # create a queue that produces filenames to read\n",
        "  # (4) filename queue\n",
        "  file_queue = tf.train.string_input_producer(training_files)\n",
        "  # (5) + (6)\n",
        "  cifar10_record = read_cifar10(file_queue)\n",
        "  # (7) image preprocessing for training\n",
        "  height = IMAGE_SIZE_CROPPED\n",
        "  width = IMAGE_SIZE_CROPPED\n",
        "  float_image = tf.cast(cifar10_record.image, tf.float32)\n",
        "  distorted_image = tf.random_crop(float_image, [height, width, 3])\n",
        "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
        "  distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
        "  distorted_image = tf.image.random_contrast(\n",
        "      distorted_image, lower=0.2, upper=1.8)\n",
        "  # standardization: subtract off the mean and divide by the variance of the pixels\n",
        "  distorted_image = tf.image.per_image_standardization(distorted_image)\n",
        "  # Set the shapes of tensors.\n",
        "  distorted_image.set_shape([height, width, 3])\n",
        "  cifar10_record.label.set_shape([1])\n",
        "  # ensure a level of mixing of elements.\n",
        "  min_fraction_of_examples_in_queue = 0.4\n",
        "  min_queue_examples = int(\n",
        "      NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue)\n",
        "  # (8) example queue\n",
        "  # Filling queue with min_queue_examples CIFAR images before starting to train\n",
        "  image_batch, label_batch = tf.train.shuffle_batch(\n",
        "      [distorted_image, cifar10_record.label],\n",
        "      batch_size=batch_size,\n",
        "      num_threads=16,\n",
        "      capacity=min_queue_examples + 3 * batch_size,\n",
        "      min_after_dequeue=min_queue_examples)\n",
        "  return image_batch, tf.reshape(label_batch, [batch_size])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-StBv6SZd-1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_input(testing_files, batch_size):\n",
        "  for f in testing_files:\n",
        "    if not tf.gfile.Exists(f):\n",
        "      raise ValueError('Failed to find file: ' + f)\n",
        "  # create a queue that produces filenames to read\n",
        "  file_queue = tf.train.string_input_producer(testing_files)\n",
        "  cifar10_record = read_cifar10(file_queue)\n",
        "  # image preprocessing for training\n",
        "  height = IMAGE_SIZE_CROPPED\n",
        "  width = IMAGE_SIZE_CROPPED\n",
        "  float_image = tf.cast(cifar10_record.image, tf.float32)\n",
        "  resized_image = tf.image.resize_image_with_crop_or_pad(\n",
        "      float_image, height, width)\n",
        "  image_eval = tf.image.per_image_standardization(resized_image)\n",
        "  image_eval.set_shape([height, width, 3])\n",
        "  cifar10_record.label.set_shape([1])\n",
        "  # Ensure that the random shuffling has good mixing properties.\n",
        "  min_fraction_of_examples_in_queue = 0.4\n",
        "  min_queue_examples = int(\n",
        "      NUM_EXAMPLES_PER_EPOCH_FOR_EVAL * min_fraction_of_examples_in_queue)\n",
        "  image_batch, label_batch = tf.train.batch(\n",
        "      [image_eval, cifar10_record.label],\n",
        "      batch_size=batch_size,\n",
        "      num_threads=16,\n",
        "      capacity=min_queue_examples + 3 * batch_size)\n",
        "  return image_batch, tf.reshape(label_batch, [batch_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhj4nUK-eHln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b744225e-542d-4ff1-984e-81a4a021a505"
      },
      "source": [
        "# test function distort_input\n",
        "with tf.Session() as sess:\n",
        "  coord = tf.train.Coordinator()\n",
        "  image, label = distort_input(training_files, BATCH_SIZE)\n",
        "  # --- Note ---\n",
        "  # If you forget to call start_queue_runners(), it will hang\n",
        "  # indefinitely and deadlock the user program.\n",
        "  # ------------\n",
        "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "  image_batch, label_batch = sess.run([image, label])\n",
        "  coord.request_stop()\n",
        "  coord.join(threads)\n",
        "  image_batch_np = np.asarray(image_batch)\n",
        "  label_batch_np = np.asarray(label_batch)\n",
        "  print('Shape of cropped image:', image.shape)\n",
        "  print('Shape of label:', label.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 12:50:51.301943 140266438244224 deprecation.py:323] From <ipython-input-25-ffb8660fa920>:20: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0729 12:50:51.321058 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0729 12:50:51.325871 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0729 12:50:51.331668 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0729 12:50:51.335419 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0729 12:50:51.343996 140266438244224 deprecation.py:323] From <ipython-input-24-bce2b8dfc6e7>:36: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
            "W0729 12:50:51.423668 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0729 12:50:51.427440 140266438244224 deprecation.py:323] From <ipython-input-25-ffb8660fa920>:48: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
            "W0729 12:50:51.445750 140266438244224 deprecation.py:323] From <ipython-input-27-f6b57dc063f7>:8: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of cropped image: (128, 24, 24, 3)\n",
            "Shape of label: (128,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al2CQj-QedRX",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow Dataset API\n",
        "to build a data input pipeline with tf.data.Dataset, here are the steps for you can follow:\n",
        "\n",
        "1. Define data source and initialize your Dataset object\n",
        "2. Apply transformations on the dataset, following are some common useful techniques\n",
        "* map\n",
        "* batch\n",
        "* shuffle\n",
        "* repeat\n",
        "3. Create iterator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZTTDA0BeaR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Construct your Dataset\n",
        "# an array with shape (8, 5)\n",
        "raw_data_a = np.array([[1, 1.1, 1.2, 1.3, 1.4], \n",
        "                     [2, 2.1, 2.2, 2.3, 2.4], \n",
        "                     [3, 3.1, 3.2, 3.3, 3.4],\n",
        "                     [4, 4.1, 4.2, 4.3, 4.4],\n",
        "                     [5, 5.1, 5.2, 5.3, 5.4],\n",
        "                     [6, 6.1, 6.2, 6.3, 6.4],\n",
        "                     [7, 7.1, 7.2, 7.3, 7.4],\n",
        "                     [8, 8.1, 8.2, 8.3, 8.4]\n",
        "                    ])\n",
        "# a list with len 8\n",
        "raw_data_b = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LxTCaxjfIZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e301269-5388-425f-c569-77fed9185f71"
      },
      "source": [
        "# thie tells the dataset that each row of raw_data_a is corresponding to each element of raw_data_b\n",
        "dataset = tf.data.Dataset.from_tensor_slices((raw_data_a, raw_data_b))\n",
        "print(dataset)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((5,), ()), types: (tf.float64, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXSCVHgefLr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0711150-a015-4061-a190-b2b1387a1c2d"
      },
      "source": [
        "#Apply transformations\n",
        "#map\n",
        "#For example, Dataset.map() provide element-wise customized data preprocessing.\n",
        "def preprocess_function(one_row_a, one_b):\n",
        "    \"\"\"\n",
        "        Input: one slice of the dataset\n",
        "        Output: modified slice\n",
        "    \"\"\"\n",
        "    # do some data preprocessing\n",
        "    # you can also input filenames and load data in here\n",
        "    \n",
        "    return tf.reshape(one_row_a, [1, 5]), tf.string_to_number(one_b)\n",
        "\n",
        "dataset = dataset.map(preprocess_function)\n",
        "print(dataset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((1, 5), ()), types: (tf.float64, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wusGD1tbfXbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shuffle\n",
        "#Dataset.shuffle maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. This way, \n",
        "#you can see your data coming with different order in different epoch. \n",
        "#This can prevent your model overfit on the order of your training data.\n",
        "dataset = dataset.shuffle(16)\n",
        "#batch\n",
        "#Now our dataset is one example by one example. \n",
        "#However, in reality, we usually want to read one batch at a time, \n",
        "#thus we can call Dataset.batch(batch_size) to stack batch_size elements together.\n",
        "#Note: Be careful that if you apply Dataset.shuffle after Dataset.batch, you'll get shuffled batch but data in a batch remains the same.\n",
        "dataset = dataset.batch(2)\n",
        "#repeat\n",
        "#Dataset.repeat(epoch) allow you iterate over a dataset in multiple epochs. \n",
        "#epoch = None will let the dataset repeats indefinitely.\n",
        "dataset = dataset.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Of1Uk3gQVL",
        "colab_type": "text"
      },
      "source": [
        "## Create Iterator and Read data\n",
        "The tf.data API currently supports the following iterators, in increasing level of sophistication:\n",
        "\n",
        "* one-shot : One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization.\n",
        "* initializable: requires you to run an explicit iterator.initializer operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more tf.placeholder() tensors that can be fed when you initialize the iterator.\n",
        "* reinitializable: can be initialized from multiple different Dataset objects. For example, if you have training data and testing data with same format.\n",
        "* feedable: can be used together with tf.placeholder to select what Iterator to use in each call to tf.Session.run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4vuauPIgPpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config = config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfExG0OegfqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9a266eac-d201-4de1-a993-6d1832fd264d"
      },
      "source": [
        "iterator_oneshot = dataset.make_one_shot_iterator()\n",
        "next_element_oneshot = iterator_oneshot.get_next()\n",
        "\n",
        "for i in range(8):\n",
        "    a, b = sess.run(next_element_oneshot)\n",
        "    print(\"Batch \", i, \", b are \", b)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 12:51:05.260281 140266438244224 deprecation.py:323] From <ipython-input-33-435ac3b96f78>:1: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch  0 , b are  [1. 2.]\n",
            "Batch  1 , b are  [4. 5.]\n",
            "Batch  2 , b are  [8. 6.]\n",
            "Batch  3 , b are  [7. 3.]\n",
            "Batch  4 , b are  [7. 3.]\n",
            "Batch  5 , b are  [5. 8.]\n",
            "Batch  6 , b are  [6. 4.]\n",
            "Batch  7 , b are  [2. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGTlleDGgiYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b89b6aca-726c-47f4-9b35-664b1da5c034"
      },
      "source": [
        "iterator_initializable = dataset.make_initializable_iterator()\n",
        "next_element_initializable = iterator_initializable.get_next()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 12:51:06.928398 140266438244224 deprecation.py:323] From <ipython-input-34-df1bbeb2c603>:1: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFcYJsuygk76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# before getting the next element, should run the initializer\n",
        "sess.run(iterator_initializable.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl2cf5N-goDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fc950902-5375-4d3b-c0b6-ecf59f530e30"
      },
      "source": [
        "for i in range(8):\n",
        "    a, b = sess.run(next_element_initializable)\n",
        "    print(\"Batch \", i, \", b are \", b)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch  0 , b are  [8. 5.]\n",
            "Batch  1 , b are  [6. 2.]\n",
            "Batch  2 , b are  [1. 3.]\n",
            "Batch  3 , b are  [4. 7.]\n",
            "Batch  4 , b are  [1. 8.]\n",
            "Batch  5 , b are  [4. 6.]\n",
            "Batch  6 , b are  [5. 2.]\n",
            "Batch  7 , b are  [3. 7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0LESIxIgszT",
        "colab_type": "text"
      },
      "source": [
        "# CNN Model for CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvogOrKhgwUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Model(object):\n",
        "  def __init__(self, model_hps):\n",
        "    self.image_size = model_hps.image_size\n",
        "    self.batch_size = model_hps.batch_size\n",
        "    self.num_classes = model_hps.num_classes\n",
        "    self.num_training_example = model_hps.num_training_example\n",
        "    self.num_epoch_per_decay = model_hps.num_epoch_per_decay\n",
        "    self.init_lr = model_hps.init_lr  # initial learn rate\n",
        "    self.moving_average_decay = model_hps.moving_average_decay\n",
        "    self.ckpt_dir = model_hps.ckpt_dir\n",
        "    \n",
        "    self.build_model()\n",
        "    \n",
        "  def build_model(self):\n",
        "    # op for training\n",
        "    self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "        \n",
        "    with tf.variable_scope('model'):\n",
        "      self.images = tf.placeholder(tf.float32,[self.batch_size, self.image_size, self.image_size, 3]) \n",
        "      self.labels = tf.placeholder(tf.int32)\n",
        "        \n",
        "      self.logits = self.inference(self.images)\n",
        "      self.top_k_op = tf.nn.in_top_k(self.logits, self.labels, 1) \n",
        "      self.total_loss = self.loss(self.logits, self.labels)\n",
        "      self.train_op = self.train(self.total_loss, self.global_step)\n",
        "    \n",
        "  def _variable_on_cpu(self, name, shape, initializer):\n",
        "    with tf.device('/cpu:0'):\n",
        "      var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
        "    \n",
        "    return var\n",
        "\n",
        "  def _variable_with_weight_decay(self, name, shape, stddev, wd=0.0):\n",
        "    \"\"\" Helper to create an initialized Variable with weight decay.\n",
        "        Note that the Variable is initialized with a truncated normal \n",
        "        distribution. A weight decay is added only if one is specified.\n",
        "        -----\n",
        "        Args:\n",
        "            name: \n",
        "                name of the variable\n",
        "            shape: \n",
        "                a list of ints\n",
        "            stddev: \n",
        "                standard deviation of a truncated Gaussian\n",
        "            wd: \n",
        "                add L2Loss weight decay multiplied by this float. If None, weight\n",
        "                decay is not added for this Variable.\n",
        "        Returns:\n",
        "            Variable Tensor\n",
        "    \"\"\"\n",
        "    initializer = tf.truncated_normal_initializer(\n",
        "        stddev=stddev, dtype=tf.float32)\n",
        "    var = self._variable_on_cpu(name, shape, initializer)\n",
        "    # deal with weight decay\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "    return var\n",
        "\n",
        "  def _conv_block(self, inp, scope, kernel_width, kernel_height, inp_channel, out_channel, strides = [1, 1, 1, 1], padding='SAME'):\n",
        "    with tf.variable_scope(scope) as scope:\n",
        "      kernel = self._variable_with_weight_decay('weights', [kernel_width, kernel_width, inp_channel, out_channel], 5e-2)\n",
        "      biases = self._variable_on_cpu('bias', [out_channel], tf.constant_initializer(0.0))\n",
        "\n",
        "      conv = tf.nn.conv2d(inp, kernel, strides=strides, padding=padding)\n",
        "      pre_activation = tf.nn.bias_add(conv, biases)\n",
        "      return tf.nn.relu(pre_activation, name=scope.name)\n",
        "\n",
        "  def _fully_connected_layer(self, inp, scope, in_dim, out_dim, relu = True):\n",
        "    with tf.variable_scope(scope) as scope:\n",
        "      weights = self._variable_with_weight_decay('weights', [in_dim, out_dim], 0.04, 0.004)\n",
        "      biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
        "      if relu:\n",
        "        return tf.nn.relu(tf.matmul(inp, weights) + biases, name=scope.name)\n",
        "      else:\n",
        "        return tf.matmul(inp, weights) + biases\n",
        "    \n",
        "  def inference(self, images):\n",
        "    \"\"\" build the model\n",
        "        -----\n",
        "        Args:\n",
        "            images with shape [batch_size,24,24,3]\n",
        "        Return:\n",
        "            logits with shape [batch_size,10]\n",
        "    \"\"\"\n",
        "    conv_1 = self._conv_block(images, 'conv_1', 5, 5, 3, 64)\n",
        "    # pool_1\n",
        "    pool_1 = tf.nn.max_pool(conv_1,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_1')\n",
        "    # norm_1 (local_response_normalization)\n",
        "    norm_1 = tf.nn.lrn(pool_1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_1')\n",
        "    \n",
        "    # conv2\n",
        "    conv_2 = self._conv_block(norm_1, 'conv_2', 5, 5, 64, 64)\n",
        "    # norm2\n",
        "    norm_2 = tf.nn.lrn(conv_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_2')\n",
        "    # pool2\n",
        "    pool_2 = tf.nn.max_pool(norm_2,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_2')\n",
        "    \n",
        "    # Flatten feature maps before fully connected layers\n",
        "    flat_features = tf.reshape(pool_2, [self.batch_size, -1])\n",
        "    dim = flat_features.get_shape()[1].value\n",
        "    # FC_1 (fully-connected layer)\n",
        "    fc_1 = self._fully_connected_layer(flat_features, 'fc1', dim, 384)\n",
        "\n",
        "    # FC_2\n",
        "    fc_2 = self._fully_connected_layer(fc_1, 'fc2', 384, 192)\n",
        "\n",
        "    logits = self._fully_connected_layer(fc_2, 'softmax_linear', 192, self.num_classes, relu = False)\n",
        "    return logits\n",
        "\n",
        "  def loss(self, logits, labels):\n",
        "    '''calculate the loss'''\n",
        "    labels = tf.cast(labels, tf.int64)\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "    tf.add_to_collection('losses', cross_entropy_mean)\n",
        "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
        "    # decay terms (L2 loss).\n",
        "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
        "\n",
        "  def train(self, total_loss, global_step):\n",
        "    '''Return training operation of one step'''\n",
        "    num_batches_per_epoch = self.num_training_example / self.batch_size\n",
        "    decay_steps = int(num_batches_per_epoch * self.num_epoch_per_decay)\n",
        "    # Decay the learning rate exponentially based on the number of steps.\n",
        "    lr = tf.train.exponential_decay(\n",
        "        self.init_lr, global_step, decay_steps, decay_rate=0.1, staircase=True)\n",
        "    opt = tf.train.GradientDescentOptimizer(lr)\n",
        "    grads = opt.compute_gradients(total_loss)\n",
        "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
        "    # Track the moving averages of all trainable variables.\n",
        "    # This step just records the moving average weights but not uses them\n",
        "    ema = tf.train.ExponentialMovingAverage(self.moving_average_decay,\n",
        "                                            global_step)\n",
        "    self.ema = ema\n",
        "    variables_averages_op = ema.apply(tf.trainable_variables())\n",
        "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
        "      train_op = tf.no_op(name='train')\n",
        "    return train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL6vHNehg2Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_hps_cifar = tf.contrib.training.HParams(\n",
        "  image_size = IMAGE_SIZE_CROPPED,\n",
        "  batch_size = BATCH_SIZE,\n",
        "  num_classes = NUM_CLASSES,\n",
        "  num_training_example = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
        "  num_epoch_per_decay = 350.0,\n",
        "  init_lr = 0.1,\n",
        "  moving_average_decay = 0.9999,\n",
        "  ckpt_dir = './model/'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zss_ExHBg7Pn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1dcaa472-1bb4-4223-ab15-d0ab2b5be67b"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# CNN model\n",
        "model = CNN_Model(model_hps_cifar)\n",
        "# Here we use CPU to handle the input because we want GPU to only focus on training.\n",
        "with tf.device('/cpu:0'):\n",
        "  data_train = distort_input(training_files, BATCH_SIZE)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 12:51:16.608037 140266438244224 deprecation.py:323] From <ipython-input-37-bb039c695d90>:16: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0729 12:51:16.622698 140266438244224 deprecation.py:506] From <ipython-input-37-bb039c695d90>:52: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0729 12:51:16.844196 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnLjc-VEhANs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(model, data_train , num_epoch):\n",
        "  saver = tf.train.Saver()\n",
        "  with tf.Session() as sess:\n",
        "    ckpt = tf.train.get_checkpoint_state(model.ckpt_dir)\n",
        "    if (ckpt and ckpt.model_checkpoint_path):\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
        "      gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
        "      sess.run(tf.assign(model.global_step, gs))\n",
        "    else:\n",
        "      # no checkpoint found\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "    model.loss_each_epoch = []\n",
        "    \n",
        "    num_batch_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN//model.batch_size\n",
        "    #start training\n",
        "    for i in range(num_epoch):\n",
        "      _loss = []\n",
        "      for _ in range(num_batch_per_epoch):\n",
        "        images, labels = sess.run(data_train)\n",
        "        \n",
        "        l, _ = sess.run([model.total_loss, model.train_op], feed_dict = {model.images:images, model.labels:labels})\n",
        "        _loss.append(l)\n",
        "      loss_this_epoch = np.sum(_loss)\n",
        "      gs = model.global_step.eval()\n",
        "      # print('loss of epoch %d: %f' % (gs / num_batch_per_epoch, loss_this_epoch))\n",
        "      model.loss_each_epoch.append(loss_this_epoch)\n",
        "      saver.save(sess, model.ckpt_dir + 'model.ckpt', global_step=gs)\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)\n",
        "  print('Done training %d epochs' %num_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EguCBbc7hAvZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b567f2b3-af60-4f7f-fb69-ee39ceab1c70"
      },
      "source": [
        "run_training(model, data_train, 180)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 13:12:44.896154 140266438244224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUIYEhaXhDPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Because now the weights are not moving average weights, we need to manually change this.\n",
        "def run_testing(model, data_test):\n",
        "  variables_to_restore = model.ema.variables_to_restore()\n",
        "  saver = tf.train.Saver(variables_to_restore)\n",
        "    \n",
        "  with tf.Session() as sess:\n",
        "    # Restore variables from disk.\n",
        "    ckpt = tf.train.get_checkpoint_state(model.ckpt_dir)\n",
        "        \n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      coord = tf.train.Coordinator()\n",
        "      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "        \n",
        "      num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // model.batch_size\n",
        "      total_sample_count = num_iter * model.batch_size\n",
        "      true_count = 0\n",
        "      for _ in range(num_iter):\n",
        "        images, labels = sess.run(data_test) \n",
        "        \n",
        "        predictions = sess.run(model.top_k_op, feed_dict = {model.images:images, model.labels:labels})\n",
        "        true_count += np.sum(predictions)\n",
        "      print('Accurarcy: %d/%d = %f' % (true_count, total_sample_count,\n",
        "                                         true_count / total_sample_count))\n",
        "      coord.request_stop()\n",
        "      coord.join(threads)\n",
        "    else:\n",
        "      print('train first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUt9u6DEhMrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test = distort_input(testing_files, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-9fwpsMhP2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_testing(model, data_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}