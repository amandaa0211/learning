{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L13-2(未完全懂)_Sequence-to-Sequence&Neural Machine Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTQmYn699Y27",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq\n",
        "Seq2Seq is constructed of two RNNs, encoder and decoder. Encoder encodes input sequence into a hidden vector first, and decoder will gererate words one by one based on it.\n",
        "\n",
        "# Neural Machine Translation\n",
        "Neural Machine Translation (NMT) is one of NLP tasks, which uses deep learning to solve language translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6qD923K9ZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
        "import numpy as np\n",
        "\n",
        "en_corpus = np.load('./dataset/translate/enCorpus.npy')\n",
        "en_vocab = np.load('./dataset/translate/enVocab.npy').tolist() # use tolist() to transform back to dict()\n",
        "en_rev = np.load('./dataset/translate/enRev.npy').tolist()\n",
        "\n",
        "ch_corpus = np.load('./dataset/translate/chCorpus.npy')\n",
        "ch_vocab = np.load('./dataset/translate/chVocab.npy').tolist()\n",
        "ch_rev = np.load('./dataset/translate/chRev.npy').tolist()\n",
        "\n",
        "for i in range(4):\n",
        "    print(' '.join([en_rev[en_corpus[i][j]]  for j in range(len(en_corpus[i]))]))\n",
        "    print(' '.join([ch_rev[ch_corpus[i][j]]  for j in range(len(ch_corpus[i]))]))\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxkqrBgD_LkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For the presentation issue, we only preserve sentences without 'UNK' token.\n",
        "en_corpus_clean = []\n",
        "ch_corpus_clean = []\n",
        "\n",
        "for i in range(len(en_corpus)):\n",
        "    if not(en_vocab['<UNK>'] in en_corpus[i] or ch_vocab['<UNK>'] in ch_corpus[i]): # remove '<UNK>' sentence\n",
        "        en_corpus_clean.append(en_corpus[i])\n",
        "        ch_corpus_clean.append(ch_corpus[i])\n",
        "\n",
        "for i in range(4):\n",
        "    print(' '.join([en_rev[en_corpus_clean[i][j]]  for j in range(len(en_corpus_clean[i]))]))\n",
        "    print(' '.join([ch_rev[ch_corpus_clean[i][j]]  for j in range(len(ch_corpus_clean[i]))]))\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RtblLc__YxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_corpus_clean = []\n",
        "ch_corpus_clean = []\n",
        "\n",
        "for i in range(len(en_corpus)):\n",
        "    if not(en_vocab['<UNK>'] in en_corpus[i] or ch_vocab['<UNK>'] in ch_corpus[i]): # remove '<UNK>' sentence\n",
        "        en_corpus_clean.append(en_corpus[i])\n",
        "        ch_corpus_clean.append(ch_corpus[i])\n",
        "\n",
        "for i in range(4):\n",
        "    print(' '.join([en_rev[en_corpus_clean[i][j]]  for j in range(len(en_corpus_clean[i]))]))\n",
        "    print(' '.join([ch_rev[ch_corpus_clean[i][j]]  for j in range(len(ch_corpus_clean[i]))]))\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmgtKyMt_v6O",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Translation Batch\n",
        "In this lab, we use legacy_seq2seq which provides a fasy way to build up seq2seq model with attention. Be careful that legacy_seq2seq is time major which means the input and output should be a list contains word batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSrWWTwt_hRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_max_len = 0\n",
        "ch_max_len = 0\n",
        "\n",
        "for i in range(len(en_corpus_clean)): # caculate max length\n",
        "    en_max_len = max(en_max_len, len(en_corpus_clean[i]))\n",
        "    ch_max_len = max(ch_max_len, len(ch_corpus_clean[i]))\n",
        "\n",
        "print(en_max_len, ch_max_len)\n",
        "#OUTPUT:\n",
        "#32 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8XqH4VIAAWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
        "        assert len(en_corpus) == len(ch_corpus)\n",
        "        \n",
        "        batch_num = len(en_corpus)//batch_size\n",
        "        n = batch_num*batch_size\n",
        "        \n",
        "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
        "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
        "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
        "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
        "        \n",
        "        self.en_max_len = en_max_len\n",
        "        self.ch_max_len = ch_max_len\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        for b in range(batch_num):\n",
        "            for i in range(b*batch_size, (b+1)*batch_size):\n",
        "                for j in range(len(en_corpus[i])-2):\n",
        "                    self.xs[j][i] = en_corpus[i][j+1]\n",
        "                for j in range(j+1, en_max_len):\n",
        "                    self.xs[j][i] = en_pad\n",
        "                \n",
        "                for j in range(len(ch_corpus[i])-1):\n",
        "                    self.ys[j][i] = ch_corpus[i][j]\n",
        "                    self.gs[j][i] = ch_corpus[i][j+1]\n",
        "                    self.ws[j][i] = 1.0\n",
        "                for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
        "                    self.ys[j][i] = ch_pad\n",
        "                    self.gs[j][i] = ch_pad\n",
        "                    self.ws[j][i] = 0.0\n",
        "    \n",
        "    def get(self, batch_id):\n",
        "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
        "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
        "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
        "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
        "        \n",
        "        return x, y, g, w\n",
        "\n",
        "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
        "                       en_vocab['<PAD>'], ch_vocab['<PAD>'], en_max_len, ch_max_len, 4)\n",
        "\n",
        "print(\"Encoder input\")\n",
        "print(\"Decoder input\")\n",
        "print(\"Decoder output\")\n",
        "print()\n",
        "\n",
        "x, y, g, w = batch.get(2)\n",
        "for i in range(4):\n",
        "    print(' '.join([en_rev[x[j][i]] for j in range(en_max_len)]))\n",
        "    print(' '.join([ch_rev[y[j][i]] for j in range(ch_max_len)]))\n",
        "    print(' '.join([ch_rev[g[j][i]] for j in range(ch_max_len)]))\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnGjCIyHAOhd",
        "colab_type": "text"
      },
      "source": [
        "## Build Seq2Seq Graph\n",
        "For training Seq2Seq, we usually use a trick named **teacher forcing** which can help training more efficiently. But when testing, there isn't teacher any more. In tensorflow implementation, we need to **build 2 same models** with one feeding previous. Since they both **share same weight**, don't forget **reuse RNN cell and model in variable scope**. **Attention mechanim** let decoder **focus on specific input** when deciding, and generate more accurate output. Thanks to legacy_seq2seq, attention has been implemented also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS0TaAEOAWYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "class MachineTranslationSeq2Seq:\n",
        "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
        "        self.en_max_len = en_max_len\n",
        "        self.ch_max_len = ch_max_len\n",
        "        \n",
        "        with tf.variable_scope('seq2seq_intput/output'):\n",
        "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
        "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
        "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
        "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
        "            \n",
        "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
        "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
        "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
        "                                                                                    self.out_cell, \n",
        "                                                                                    en_size, ch_size, 300)\n",
        "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
        "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
        "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
        "                                                                                        self.pred_cell, \n",
        "                                                                                        en_size, ch_size, 300, \n",
        "                                                                                        feed_previous=True)\n",
        "        \n",
        "        with tf.variable_scope('loss'):\n",
        "            # caculate weighted loss\n",
        "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
        "                                                                                          self.groundtruths, \n",
        "                                                                                          self.weights))\n",
        "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
        "        \n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config)\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def train(self, x, y, g, w):\n",
        "        fd = {}\n",
        "        for i in range(self.en_max_len):\n",
        "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
        "        \n",
        "        for i in range(self.ch_max_len):\n",
        "            fd[self.dec_inputs[i]] = y[i]\n",
        "            fd[self.groundtruths[i]] = g[i]\n",
        "            fd[self.weights[i]] = w[i]\n",
        "        \n",
        "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def output(self, x, y):\n",
        "        fd = {}\n",
        "        for i in range(self.en_max_len):\n",
        "            fd[self.enc_inputs[i]] = x[i]\n",
        "        \n",
        "        for i in range(self.ch_max_len):\n",
        "            fd[self.dec_inputs[i]] = y[i]\n",
        "        \n",
        "        out = self.sess.run(self.outputs, fd)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def predict(self, x, ch_beg):\n",
        "        fd = {}\n",
        "        for i in range(self.en_max_len):\n",
        "            fd[self.enc_inputs[i]] = x[i]\n",
        "        \n",
        "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
        "            if i==0:\n",
        "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
        "            else:\n",
        "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
        "        \n",
        "        pd = self.sess.run(self.predictions, fd)\n",
        "        \n",
        "        return pd\n",
        "    \n",
        "    def save(self, e):\n",
        "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
        "    \n",
        "    def restore(self, e):\n",
        "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5_4ZyvTDT9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "model = MachineTranslationSeq2Seq(en_max_len, ch_max_len, len(en_vocab), len(ch_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBlcZkCrDbYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 256\n",
        "batch_num = len(en_corpus_clean)//BATCH_SIZE\n",
        "\n",
        "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
        "                       en_vocab['<PAD>'], ch_vocab['<PAD>'], \n",
        "                       en_max_len, ch_max_len, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLzfxfqHDgRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rec_loss = []\n",
        "for e in range(EPOCHS):\n",
        "    train_loss = 0\n",
        "    \n",
        "    for b in range(batch_num):\n",
        "        x, y, g, w = batch.get(b)\n",
        "        batch_loss = model.train(x, y, g, w)\n",
        "        train_loss += batch_loss\n",
        "    \n",
        "    train_loss /= batch_num\n",
        "    rec_loss.append(train_loss)\n",
        "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
        "    \n",
        "    model.save(e)\n",
        "    \n",
        "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpwuDcF_DoND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rec_loss = np.load('./model/seq2seq/rec_loss.npy')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt_loss = plt.plot([rec_loss[i] for i in range(len(rec_loss))])\n",
        "plt.legend(['Loss'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKiU2bGYDoZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.restore(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ4xc_IXD2D9",
        "colab_type": "text"
      },
      "source": [
        "# Cherry Pick and Show Result\n",
        "**BLEU is a metric for supervised text generation which finds the similarity between two sentence based on n-gram token.** Now, we want to show some great translation result which is called **cherry pick**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jY-b_eXEB8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "def cherry_pick(records, n, upper_bound=1.0):\n",
        "    bleus = []\n",
        "    \n",
        "    for en, ch_gr, ch_pd in records:\n",
        "        # caculate BLEU by nltk\n",
        "        bleu = nltk.translate.bleu_score.sentence_bleu([ch_gr], ch_pd)\n",
        "        bleus.append(bleu)\n",
        "    \n",
        "    lst = [i for i in range(len(records)) if bleus[i]<=upper_bound]\n",
        "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True) # sort by BLEU score\n",
        "    \n",
        "    return [records[lst[i]] for i in range(n)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmhqBV_jEPV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rd\n",
        "\n",
        "records = []\n",
        "\n",
        "for i in range(10):\n",
        "    i = rd.randint(0, batch_num-1) # random pick one to translate\n",
        "    \n",
        "    x, y, g, w = batch.get(i)\n",
        "    out = model.output(x, y)\n",
        "    pd = model.predict(x, ch_vocab['<BEG>'])\n",
        "\n",
        "    for j in range(10):\n",
        "        j = rd.randint(0, BATCH_SIZE-1)\n",
        "        \n",
        "        en = [en_rev[x[i][j]] for i in range(en_max_len)]\n",
        "        en = en[:en.index('<PAD>')]\n",
        "        ch_gr = [ch_rev[g[i][j]] for i in range(ch_max_len)]\n",
        "        if '<END>' in ch_gr:\n",
        "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
        "        ch_pd = [ch_rev[np.argmax(pd[i][j, :])] for i in range(ch_max_len)]\n",
        "        if '<END>' in ch_pd:\n",
        "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
        "        \n",
        "        records.append([en, ch_gr, ch_pd])\n",
        "\n",
        "n = 12 # how many result we show\n",
        "rec_cherry = cherry_pick(records, n)\n",
        "\n",
        "print(\"Encoder input\")\n",
        "print(\"Ground truth\")\n",
        "print(\"Decoder output\")\n",
        "print()\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(3):\n",
        "        print(' '.join(rec_cherry[i][j]))\n",
        "    \n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}