{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W12-2(未完全理解)_Visualization & Style Transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EBLphnVi8_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipyd\n",
        "import scipy.misc\n",
        "from libs import utils\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ZzL7vYjSoJ",
        "colab_type": "text"
      },
      "source": [
        "# Visualize Convolutional Neural Networks\n",
        "We are going to visualize a neural network pretrained using ImageNet, a large dataset used in ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The training dataset contains around 1.2 million images composed of 1000 different types of objects. The pretrained network learned how to create useful representations of the data to differentiate between different classes.\n",
        "\n",
        "## Loading a Pretrained Network\n",
        "We can use a model that has been already trained (\"pretrained\") by someone else. We just need to have access to the model's parameters. Fortunately, nowadays many researchers are sharing their pretrained models. This is very convenient because it saves us a lot of time to train. \n",
        "\n",
        "Here we are going to use a pretrained VGG19 model. This is an architecture introduced from this paper. This model is known for its simplicity, using only 3×3 convolutional layers stacked on top of each other in increasing depth. The \"19\" in its name stands for the number of layers in the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNzp_41i-Epo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('vgg19.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ISrbY0e9887",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from libs import vgg19\n",
        "\n",
        "#start an interactive session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "images = tf.placeholder(tf.float32, [1, 224, 224, 3])\n",
        "train_mode = tf.placeholder(tf.bool)\n",
        "\n",
        "#load the model\n",
        "vgg = vgg19.Vgg19()\n",
        "vgg.build(images, train_mode)\n",
        "\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npHIGX8FjifN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = tf.get_default_graph()\n",
        "names = [op.name for op in g.get_operations()]\n",
        "print('Sample of available operations: \\n',names[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr-FNTGLjlGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The input to the graph is stored in the first tensor output, \n",
        "#and the probability of the 1000 possible objects is in the last probability layer:\n",
        "input_name = names[0] + ':0'\n",
        "x = g.get_tensor_by_name(input_name)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkxgaDfdjuTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax = g.get_tensor_by_name(names[-2] + ':0')\n",
        "#or use this: softmax = vgg.prob\n",
        "softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yye2A5vYjxbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_img = utils.load_image('wonder-woman.jpg')\n",
        "\n",
        "plt.imshow(processed_img)\n",
        "print('image shape: ', processed_img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCfdPrkWj40o",
        "colab_type": "text"
      },
      "source": [
        "Our images must be shaped as a 4-dimensional shape describing the number of images, height, width, and number of channels before being fed into the network. So our original 3-dimensional image of height, width, channels needs an additional dimension on the 0th axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boSkINAyj5hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_img_4d = processed_img[np.newaxis]\n",
        "print(processed_img_4d.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKQgo5YIj87G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = np.squeeze(softmax.eval(feed_dict={images: processed_img_4d, train_mode:False}))\n",
        "utils.print_prob(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE7tLkoukD7h",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Filters\n",
        "Let's try to first visualize the weights of the convolution filters to somehow help us understand what is happening inside the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sv8fEFpkKFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_vgg = vgg.data_dict['conv1_1'][0]\n",
        "print(W_vgg.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip1Ao7xUkMg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_montage = utils.montage_filters(W_vgg)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(W_montage, interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59IncxUUkSvX",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Convolutional Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdzQikyhkUj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_conv1_1 = vgg.conv1_1.eval(feed_dict={images: processed_img_4d, train_mode:False}) \n",
        "vgg_conv2_1 = vgg.conv2_1.eval(feed_dict={images: processed_img_4d, train_mode:False})\n",
        "vgg_conv5_1 = vgg.conv5_1.eval(feed_dict={images: processed_img_4d, train_mode:False})\n",
        "feature = vgg_conv1_1\n",
        "montage = utils.montage_filters(np.rollaxis(np.expand_dims(feature[0], 3), 3, 2))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(montage, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rtt_EEnkYdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = vgg_conv2_1\n",
        "montage = utils.montage_filters(np.rollaxis(np.expand_dims(feature[0], 3), 3, 2))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(montage, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORacYUYnkdOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_shape = tf.shape(feature).eval(feed_dict={images:processed_img_4d, train_mode:False})\n",
        "print(layer_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAs-8WjukgnC",
        "colab_type": "text"
      },
      "source": [
        "Our original image which was 1 x 224 x 224 x 3 color channels, now has 128 new channels of information. Some channels capture edges of the body, some capture the face.\n",
        "\n",
        "We can also try to visualize some features from higher levels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5okbt7Wkdto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = vgg_conv5_1\n",
        "montage = utils.montage_filters(np.rollaxis(np.expand_dims(feature[0], 3), 3, 2))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(montage, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj47KVzJkm_T",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Gradient\n",
        "Visualizing convolutional output is a pretty useful technique for visualizing shallow convolution layers, but when we get to the deeper layers we have many different channels of information being fed to deeper convolution filters of some very high dimensions. It's hard to understand them just by just looking at the convolution output.\n",
        "\n",
        "If we want to understand what the deeper layers are really doing, we can try to use backpropagation to show us the gradients of a particular neuron with respect to our input image. Let's visualize the network's gradient when backpropagated to the original input image. This is telling us which pixels are responding to the predicted class or given neuron.\n",
        "\n",
        "We will make a forward pass up to the layer that we are interested in, and then backpropagate to help us understand which pixels contributed the most to the final activation of that layer.\n",
        "\n",
        "We first create an operation which will find the maximum neuron of all activations in a layer, and then calculate the gradient of that objective with respect to the input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MakkXBVMkkM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = vgg.conv4_2\n",
        "gradient = tf.gradients(tf.reduce_max(feature, axis=3), images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O-aR0Zmk1pv",
        "colab_type": "text"
      },
      "source": [
        "When we run this network now, we will specify the gradient operation we've created, instead of the softmax layer of the network. This will run a forward prop up to the layer we asked to find the gradient with, and then run a back prop all the way to the input image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cik4Fqozk2St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = sess.run(gradient, feed_dict={images: processed_img_4d, train_mode:True})[0]\n",
        "#look at the range of values\n",
        "print(np.min(res[0]), np.max(res[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVcOuOfk8XN",
        "colab_type": "text"
      },
      "source": [
        "It will be hard to understand the gradient in that range of values. What we can do is normalize the gradient in a way that lets us see it more in terms of the normal range of color values. After normalizing the gradient values, let's visualize the original image and the output of the backpropagated gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ylXRgnfk5Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res_normalized = utils.normalize(res)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "plt.figure(figsize=(10,10))\n",
        "axs[0].imshow(processed_img)\n",
        "axs[1].imshow(res_normalized[0])\n",
        "#We can see that the edges of wonder woman triggers the neurons the most!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl5Z4q5Gk_ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's create utility functions which will help us visualize any single neuron in a layer.\n",
        "def compute_gradient_single_neuron(feature, neuron_i):\n",
        "  '''visualize a single neuron in a layer, with neuron_i specifying the index of the neuron'''\n",
        "  gradient = tf.gradients(tf.reduce_mean(feature[:, :, :, neuron_i]), images)\n",
        "  res = sess.run(gradient, feed_dict={images: processed_img_4d, train_mode: False})[0]\n",
        "  return res\n",
        "gradient = compute_gradient_single_neuron(vgg.conv5_2, 77)\n",
        "gradient_norm = utils.normalize(gradient)\n",
        "montage = utils.montage(np.array(gradient_norm))\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(processed_img)\n",
        "axs[1].imshow(montage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03sIXL5OlQnq",
        "colab_type": "text"
      },
      "source": [
        "# A Neural Algorithm of Artistic Style\n",
        "## Defining Content Features and Style Features\n",
        "* Content features of the content image is calculated by feeding the content image into the neural network, and extract the activations of those CONTENT_LAYERS. \n",
        "* For style features, we extract the correlation of the features of the style-image layer-wise (the gram matrix). By adding up the feature correlations of multiple layers, we obtain a multi-scale representation of the input image, which captures its texture information instead of the object arrangement in the input image.\n",
        "Given the content features and the stlye features, we can design a loss function that makes the final image contains the content but are illustrated in the style of the style-image.\n",
        "\n",
        "## Defining the Loss\n",
        "Our goal is to create an output image which is synthesized by finding an image that simultaneously matches the content features of the photograph and the style features of the respective piece of art. How can we do that? We can define the loss function as the composition of:\n",
        "\n",
        "1. The dissimilarity of the content features between the output image and the content image; and\n",
        "2. The dissimilarity of the style features between the output image and the style image to the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS1dTWamlvnS",
        "colab_type": "text"
      },
      "source": [
        "We start with a noisy initial image, then set it as tensorflow Variable, and instead of doing gradient descent on the weight, we fix the weight and do gradient descent on the initial image to minimize the loss function (which is the sum of style loss and content loss). \n",
        "\n",
        "It might be easier for you to understand through code. Let's start by preparing our favorite content image and style image from some great artists. Let's continue using wonder woman as the content image simply because she is awesome! For the style image let's use Van Gogh's classic work Starry Night. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ7zwRUYlMOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "content_directory = 'contents/'\n",
        "style_directory = 'styles/'\n",
        "\n",
        "# This is the directory to store the final stylized images\n",
        "output_directory = 'image_output/'\n",
        "if not os.path.exists(output_directory):\n",
        "  os.makedirs(output_directory)\n",
        "    \n",
        "# This is the directory to store the half-done images during the training.\n",
        "checkpoint_directory = 'checkpoint_output/'\n",
        "if not os.path.exists(checkpoint_directory):\n",
        "  os.makedirs(checkpoint_directory)\n",
        "    \n",
        "content_path = os.path.join(content_directory, 'wonder-woman.jpg')\n",
        "style_path = os.path.join(style_directory, 'starry-night.jpg')\n",
        "output_path = os.path.join(output_directory, 'wonder-woman-starry-night-iteration-1000.jpg')\n",
        "\n",
        "# please notice that the checkpoint_images_path has to contain %s in the file_name\n",
        "checkpoint_path = os.path.join(checkpoint_directory, 'wonder-woman-starry-night-iteration-1000-%s.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9krvUPxNl2dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utility functions for loading the convolution layers of VGG19 model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "VGG_MEAN = [103.939, 116.779, 123.68]\n",
        "\n",
        "VGG19_LAYERS = (\n",
        "  'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "  'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "\n",
        "  'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "  'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "  'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "  'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "  'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "'relu5_3', 'conv5_4', 'relu5_4'\n",
        ")\n",
        "\n",
        "\n",
        "def net_preloaded(input_image, pooling):\n",
        "  data_dict = np.load('libs/vgg19.npy', encoding='latin1').item()\n",
        "  net = {}\n",
        "  current = input_image\n",
        "  for i, name in enumerate(VGG19_LAYERS):\n",
        "    kind = name[:4]\n",
        "    if kind == 'conv':\n",
        "      kernels = get_conv_filter(data_dict, name)\n",
        "      # kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "\n",
        "      bias = get_bias(data_dict, name)\n",
        "      # matconvnet: weights are [width, height, in_channels, out_channels]\n",
        "      # tensorflow: weights are [height, width, in_channels, out_channels]\n",
        "\n",
        "      # bias = bias.reshape(-1)\n",
        "      current = conv_layer(current, kernels, bias)\n",
        "    elif kind == 'relu':\n",
        "      current = tf.nn.relu(current)\n",
        "    elif kind == 'pool':\n",
        "      current = pool_layer(current, pooling)\n",
        "    \n",
        "    net[name] = current\n",
        "\n",
        "  assert len(net) == len(VGG19_LAYERS)\n",
        "  return net\n",
        "\n",
        "def conv_layer(input, weights, bias):\n",
        "  conv = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='SAME')\n",
        "  return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "\n",
        "def pool_layer(input, pooling):\n",
        "  if pooling == 'avg':\n",
        "    return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
        "            padding='SAME')\n",
        "  else:\n",
        "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
        "            padding='SAME')\n",
        "\n",
        "# before we feed the image into the network, we preprocess it by \n",
        "# extracting the mean_pixel from it.\n",
        "def preprocess(image):\n",
        "  return image - VGG_MEAN\n",
        "\n",
        "# remember to unprocess it before you plot it out and save it.\n",
        "def unprocess(image):\n",
        "  return image + VGG_MEAN\n",
        "\n",
        "def get_conv_filter(data_dict, name):\n",
        "  return tf.constant(data_dict[name][0], name=\"filter\")\n",
        "\n",
        "def get_bias(data_dict, name):\n",
        "  return tf.constant(data_dict[name][1], name=\"biases\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlVApkrml9HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Styler Transfer Algorithm\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from PIL import Image\n",
        "\n",
        "# feel free to try different layers\n",
        "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
        "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
        "\n",
        "VGG_MEAN = [103.939, 116.779, 123.68]\n",
        "\n",
        "def stylize(content, styles, network_path='libs/imagenet-vgg-verydeep-19.mat', \n",
        "            iterations=1000, content_weight=5e0, content_weight_blend=0.5, style_weight=5e2, \n",
        "            style_layer_weight_exp=1,style_blend_weights=None, tv_weight=100,\n",
        "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
        "            print_iterations=100, checkpoint_iterations=100, checkpoint_path=None,\n",
        "            output_path=None):\n",
        "    \n",
        "    \n",
        "  shape = (1,) + content.shape                             #content image shape : (1,433,770,3)\n",
        "  style_shapes = [(1,) + style.shape for style in styles]  #style image shape : (1,600,800,3)\n",
        "  content_features = {}\n",
        "  style_features = [{} for _ in styles]\n",
        "\n",
        "    \n",
        "  # scale the importance of each style layers according to their depth. \n",
        "  # (deeper layers are more important if style_layers_weights > 1 (default = 1))\n",
        "  layer_weight = 1.0\n",
        "  style_layers_weights = {}                                # weight for different network layers\n",
        "  for style_layer in STYLE_LAYERS:                                    \n",
        "    style_layers_weights[style_layer] = layer_weight       #'relu1_1','relu2_1',...,'relu5_1'\n",
        "    layer_weight *= style_layer_weight_exp                 # 1.0\n",
        "\n",
        "        \n",
        "  # normalize style layer weights\n",
        "  layer_weights_sum = 0\n",
        "  for style_layer in STYLE_LAYERS:                         #'relu1_1',..., 'relu5_1'\n",
        "    layer_weights_sum += style_layers_weights[style_layer] # 5.0\n",
        "  for style_layer in STYLE_LAYERS:\n",
        "    style_layers_weights[style_layer] /= layer_weights_sum\n",
        "\n",
        "        \n",
        "  # FEATURE MAPS FROM CONTENT IMAGE\n",
        "  # compute the feature map of the content image by feeding it into the network\n",
        "  #the output net contains the features of each content layer\n",
        "  g = tf.Graph()\n",
        "  with g.as_default(), tf.Session() as sess:\n",
        "    image = tf.placeholder('float', shape=shape)\n",
        "\n",
        "    net = net_preloaded(image, pooling)             # {'conv1_1':Tensor,relu1_1:Tensor...}\n",
        "    content_pre = np.array([preprocess(content)])   # (1,433,770,3) subtract the mean pixel\n",
        "    for layer in CONTENT_LAYERS:                    #'relu4_2', 'relu5_2'\n",
        "      content_features[layer] = net[layer].eval(feed_dict={image: content_pre})  \n",
        "\n",
        "            \n",
        "  # FEATURE MAPS (GRAM MATRICES) FROM STYLE IMAGE\n",
        "  # compute style features of the style image by feeding it into the network\n",
        "  # and calculate the gram matrix\n",
        "  for i in range(len(styles)):\n",
        "    g = tf.Graph()\n",
        "    with g.as_default(), tf.Session() as sess:\n",
        "      image = tf.placeholder('float', shape=style_shapes[i])\n",
        "      net = net_preloaded(image, pooling)                           \n",
        "      style_pre = np.array([preprocess(styles[i])])\n",
        "      for layer in STYLE_LAYERS:              #'relu1_1', 'relu2_1',..., 'relu5_1'\n",
        "        features = net[layer].eval(feed_dict={image: style_pre})  # relu_1:(1,600,800,64)\n",
        "        features = np.reshape(features, (-1, features.shape[3]))  # (480000, 64)\n",
        "        gram = np.matmul(features.T, features) / features.size    # (64,64)\n",
        "        style_features[i][layer] = gram\n",
        "\n",
        "                \n",
        "  # make stylized image using backpropogation\n",
        "  with tf.Graph().as_default():\n",
        "\n",
        "    # Generate a random image (the output image) with the same shape as the content image\n",
        "    initial = tf.random_normal(shape) * 0.256  \n",
        "    image = tf.Variable(initial)\n",
        "    net = net_preloaded(image, pooling)\n",
        "    \n",
        "\n",
        "    # CONTENT LOSS\n",
        "    # we can adjust the weight of each content layers\n",
        "    # content_weight_blend is the ratio of two used content layers in this example\n",
        "    content_layers_weights = {}\n",
        "    content_layers_weights['relu4_2'] = content_weight_blend \n",
        "    content_layers_weights['relu5_2'] = 1.0 - content_weight_blend      \n",
        "\n",
        "    content_loss = 0\n",
        "    content_losses = []\n",
        "    for content_layer in CONTENT_LAYERS:\n",
        "      # Use MSE as content losses\n",
        "      # content weight is the coefficient for content loss\n",
        "      content_losses.append(content_layers_weights[content_layer] * content_weight * \n",
        "              (2 * tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) /\n",
        "              content_features[content_layer].size))\n",
        "    content_loss += reduce(tf.add, content_losses)\n",
        "\n",
        "\n",
        "\n",
        "    # STYLE LOSS\n",
        "    # We can specify different weight for different style images\n",
        "    # style_layers_weights => weight for different network layers\n",
        "    # style_blend_weights => weight between different style images\n",
        "\n",
        "    if style_blend_weights is None:\n",
        "      style_blend_weights = [1.0/len(styles) for _ in styles]\n",
        "    else:\n",
        "      total_blend_weight = sum(style_blend_weights)\n",
        "      # normalization\n",
        "      style_blend_weights = [weight/total_blend_weight\n",
        "                             for weight in style_blend_weights]\n",
        "\n",
        "\n",
        "    style_loss = 0\n",
        "    # iterate to calculate style loss with multiple style images\n",
        "    for i in range(len(styles)):\n",
        "      style_losses = []\n",
        "      for style_layer in STYLE_LAYERS:             # e.g. relu1_1\n",
        "        layer = net[style_layer]                   # relu1_1 of output image:(1,433,770,64)\n",
        "        _, height, width, number = map(lambda i: i.value, layer.get_shape())  \n",
        "        size = height * width * number\n",
        "        feats = tf.reshape(layer, (-1, number))    # (333410,64)\n",
        "\n",
        "        # Gram matrix for the features in relu1_1 of the output image.\n",
        "        gram = tf.matmul(tf.transpose(feats), feats) / size\n",
        "        # Gram matrix for the features in relu1_1 of the style image\n",
        "        style_gram = style_features[i][style_layer]   \n",
        "\n",
        "        # Style loss is the MSE for the difference of the 2 Gram matrices\n",
        "        style_losses.append(style_layers_weights[style_layer] * 2 * \n",
        "                            tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
        "      style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
        "\n",
        "\n",
        "    # TOTAL VARIATION LOSS  \n",
        "    # Total variation denoising to do smoothing; cost to penalize neighboring pixel\n",
        "    # not used by the original paper by Gatys et al\n",
        "    # According to the paper Mahendran, Aravindh, and Andrea Vedaldi. \"Understanding deep \n",
        "    # image representations by inverting them.\"\n",
        "    # Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n",
        "    tv_y_size = _tensor_size(image[:,1:,:,:])\n",
        "    tv_x_size = _tensor_size(image[:,:,1:,:])\n",
        "    tv_loss = tv_weight * 2 * (\n",
        "      (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
        "          tv_y_size) +\n",
        "      (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
        "          tv_x_size))\n",
        "\n",
        "\n",
        "    #OVERALL LOSS\n",
        "    loss = content_loss + style_loss + tv_loss\n",
        "\n",
        "    train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
        "\n",
        "    def print_progress():\n",
        "      print('     iteration: %d\\n' % i)\n",
        "      print('  content loss: %g\\n' % content_loss.eval())\n",
        "      print('    style loss: %g\\n' % style_loss.eval())\n",
        "      print('       tv loss: %g\\n' % tv_loss.eval())\n",
        "      print('    total loss: %g\\n' % loss.eval())\n",
        "\n",
        "    def imsave(path, img):\n",
        "      img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "      Image.fromarray(img).save(path, quality=95)\n",
        "\n",
        "    \n",
        "    \n",
        "    # TRAINING\n",
        "    best_loss = float('inf')\n",
        "    best = None\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "      sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "      if (print_iterations and print_iterations != 0):\n",
        "        print_progress()\n",
        "        \n",
        "      for i in range(iterations):\n",
        "\n",
        "        train_step.run()\n",
        "\n",
        "        last_step = (i == iterations - 1)\n",
        "        if last_step or (print_iterations and i % print_iterations == 0):\n",
        "          print_progress()\n",
        "\n",
        "        # store output and checkpoint images\n",
        "        if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
        "          this_loss = loss.eval()\n",
        "          if this_loss < best_loss:\n",
        "            best_loss = this_loss\n",
        "            best = image.eval()\n",
        "\n",
        "          img_out = unprocess(best.reshape(shape[1:]))\n",
        "\n",
        "          output_file = None\n",
        "          if not last_step:\n",
        "            if checkpoint_path:\n",
        "                output_file = checkpoint_path % i\n",
        "          else:\n",
        "            output_file = output_path\n",
        "\n",
        "          if output_file:\n",
        "            imsave(output_file, img_out)\n",
        "            \n",
        "  print(\"finish stylizing.\")\n",
        "\n",
        "\n",
        "\n",
        "def _tensor_size(tensor):\n",
        "  from operator import mul\n",
        "  return reduce(mul, (d.value for d in tensor.get_shape()), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVYtBoSwmI4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image = utils.loadImage(content_directory, 'wonder-woman.jpg')\n",
        "style_image = utils.loadImage(style_directory, 'starry-night.jpg')\n",
        "utils.showImage(content_image, style_image)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRB1GiEpmLXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path=None\n",
        "output_path = output_directory + 'wonder-woman-starry-night-tvweight-100.jpg'\n",
        "\n",
        "stylize(content_image, [style_image], iterations=1000,\n",
        "        content_weight=5e0, content_weight_blend=1, style_weight=5e2, \n",
        "        style_layer_weight_exp=1, style_blend_weights=None, tv_weight=100,\n",
        "        learning_rate=1e1, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
        "        print_iterations=100, checkpoint_iterations=100, checkpoint_path=checkpoint_path,\n",
        "        output_path=output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}